#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# On an existing Zeppelin/Spark cluster with AXS (see: 20201231-AXS-with-Hive.txt), try replacing with the following AXS distribution:
# https://github.com/stevenstetzler/axs/releases/download/v3.0.0-preview/axs-distribution.tgz






# -------------------------------------------------------------------
# Check that pyspark works before changing everything:
# fedora@zeppelin


pyspark
Python 3.7.7 (default, Mar 13 2020, 21:39:43) 
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/01/18 16:11:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/01/18 16:11:23 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/18 16:11:24 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.



# -------------------------------------------------------------------
# Try replacing the Spark installation in our Zeppelin node:
# fedora@zeppelin

sudo su
pushd /opt	
    mv spark-3.0.0-preview-bin-AXS-v3.0.0-preview spark-2.4.7
    wget https://github.com/stevenstetzler/axs/releases/download/v3.0.0-preview/axs-distribution.tgz
    tar -xzvf axs-distribution.tar.gz
    rm -f axs-distribution.tar.gz 
    chown -R fedora:root spark-3.0.0-preview-bin-AXS-v3.0.0-preview/

popd



# -------------------------------------------------------------------
# Copy Spark & Hive configuration from previous Spark install
# fedora@zeppelin


cat spark-defaults.conf

spark.master            yarn
spark.driver.memory              7g
spark.yarn.am.memory            7g
spark.executor.memory          7g
spark.eventLog.enabled  true
spark.driver.maxResultSize8192m
spark.local.dir         /opt/spark/local
spark.executor.cores            4
spark.executor.instances    4
spark.yarn.am.cores  4
spark.eventLog.enabled  true
spark.eventLog.dirhdfs://master01:9000/spark-log
# END Ansible managed Spark configuration
# BEGIN Ansible managed Spark environment
# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
# END Ansible managed Spark environment
spark.sql.warehouse.dir=/warehouse
spark.files.maxPartitionBytes	471859200
spark.memory.offHeap.enabled	true
spark.memory.offHeap.size	4g 
spark.jars	/opt/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar
spark.scheduler.minRegisteredResourcesRatio	0.75





# -------------------------------------------------------------------
# Try running PySpark
# fedora@zeppelin

/opt/spark/bin/pyspark

Python 3.7.7 (default, Mar 13 2020, 21:39:43) 
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/01/19 13:18:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/01/19 13:18:06 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/19 13:18:07 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.
>>> ls
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
NameError: name 'ls' is not defined
>>> from axs import AxsCatalog, Constants

>>> db = AxsCatalog(spark)
>>> 
>>> 
>>> dfgaia = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("dec>89")

>>> df2mass = spark.read.parquet("file:////user/nch/PARQUET/TESTS/2MASS/*.parquet").where("dec>88")
>>>                                                                             
>>> # drop the catalogues from previous runs if necessary:
>>> db.drop_table('gaia_source_sample')
java.lang.ClassNotFoundException: com.mysql.jdbc.Driver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.dirac.axs.util.CatalogUtils.getConnection(CatalogUtils.java:44)
	at org.dirac.axs.util.CatalogUtils.deleteTable(CatalogUtils.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
An error occurred while calling o50.deleteTable.
: java.lang.Exception: Configuration error: connection not available.
	at org.dirac.axs.util.CatalogUtils.getConnection(CatalogUtils.java:49)
	at org.dirac.axs.util.CatalogUtils.deleteTable(CatalogUtils.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)



# MySQL driver missing, copy over from previous install

cp /opt/spark-2.4.7/jars/mysql-connector-java-8.0.22.jar /opt/spark/jars/


./pyspark 
Python 3.7.7 (default, Mar 13 2020, 21:39:43) 
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/01/19 13:31:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/01/19 13:31:35 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/19 13:31:36 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.


from axs import AxsCatalog, Constants
db = AxsCatalog(spark)

dfgaia = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("dec>89")
df2mass = spark.read.parquet("file:////user/nch/PARQUET/TESTS/2MASS/*.parquet").where("dec>88")

# drop the catalogues from previous runs if necessary:
db.drop_table('gaia_source_sample')
db.drop_table('twomass_sample')


# Create new Parquet files in Spark Metastore
db.save_axs_table(dfgaia, "gaia_source_sample", repartition=True, calculate_zone=True, path = 'file:///user/nch/PARQUET/AXS/GEDR3')
db.save_axs_table(df2mass, "twomass_sample", repartition=True, calculate_zone=True, path = 'file:///user/nch/PARQUET/AXS/2MASS')

gaia = db.load('gaia_source_sample')
twomass = db.load('twomass_sample')

gaia.exclude_duplicates().count()
> 21254          

gaia_sdss_cm = gaia.crossmatch(twomass, 5.0*Constants.ONE_ASEC, return_min=False, include_dist_col=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/axs/axsframe.py", line 168, in crossmatch
    res = DataFrame(ff.crossmatch(self._jdf, axsframe._jdf, r, use_smj_optim, False, True),  # return_min, include_dist_col),
  File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/java_gateway.py", line 1286, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 98, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.8.1-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o264.crossmatch.
: java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;
	at org.dirac.axs.FrameFunctions$.crossmatch(FrameFunctions.scala:42)
	at org.dirac.axs.FrameFunctions.crossmatch(FrameFunctions.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)



# Same error we've seen before:
  > java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;


# -------------------------------------------------------------------
# Check what jars are there for this release:
# fedora@zeppelin

ls -al /opt/spark/jars
..
-rw-r--r--.  1 fedora root      41094 Apr 12  2020 hadoop-annotations-2.7.4.jar
-rw-r--r--.  1 fedora root      94621 Apr 12  2020 hadoop-auth-2.7.4.jar
-rw-r--r--.  1 fedora root      26243 Apr 12  2020 hadoop-client-2.7.4.jar
-rw-r--r--.  1 fedora root    3499224 Apr 12  2020 hadoop-common-2.7.4.jar
-rw-r--r--.  1 fedora root    8350471 Apr 12  2020 hadoop-hdfs-2.7.4.jar
-rw-r--r--.  1 fedora root     543852 Apr 12  2020 hadoop-mapreduce-client-app-2.7.4.jar
-rw-r--r--.  1 fedora root     776862 Apr 12  2020 hadoop-mapreduce-client-common-2.7.4.jar
-rw-r--r--.  1 fedora root    1558288 Apr 12  2020 hadoop-mapreduce-client-core-2.7.4.jar
-rw-r--r--.  1 fedora root      62960 Apr 12  2020 hadoop-mapreduce-client-jobclient-2.7.4.jar
-rw-r--r--.  1 fedora root      72050 Apr 12  2020 hadoop-mapreduce-client-shuffle-2.7.4.jar
-rw-r--r--.  1 fedora root    2039372 Apr 12  2020 hadoop-yarn-api-2.7.4.jar
-rw-r--r--.  1 fedora root     166121 Apr 12  2020 hadoop-yarn-client-2.7.4.jar
-rw-r--r--.  1 fedora root    1679789 Apr 12  2020 hadoop-yarn-common-2.7.4.jar
-rw-r--r--.  1 fedora root     388572 Apr 12  2020 hadoop-yarn-server-common-2.7.4.jar
-rw-r--r--.  1 fedora root      58699 Apr 12  2020 hadoop-yarn-server-web-proxy-2.7.4.jar
...

-rw-r--r--.  1 fedora root     112235 Apr 12  2020 scala-collection-compat_2.12-2.1.1.jar
-rw-r--r--.  1 fedora root   10672015 Apr 12  2020 scala-compiler-2.12.10.jar
-rw-r--r--.  1 fedora root    5276900 Apr 12  2020 scala-library-2.12.10.jar
-rw-r--r--.  1 fedora root     222980 Apr 12  2020 scala-parser-combinators_2.12-1.1.2.jar
-rw-r--r--.  1 fedora root    3678534 Apr 12  2020 scala-reflect-2.12.10.jar
-rw-r--r--.  1 fedora root     556575 Apr 12  2020 scala-xml_2.12-1.2.0.jar

..

# Hadoop version matches what is currently there: Hadoop 2.7.4
# Scala version is 2.12

