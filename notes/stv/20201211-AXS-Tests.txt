#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


# Deploy an automated ansible-based version of our cluster

# The following is run on the master node of our cluster


# Try fetching the latest tarball from here, and replacing our spark
# --------------------------------------------------------------------
# https://github.com/astronomy-commons/axs/releases

# wget https://github.com/astronomy-commons/axs/releases/download/v1.0/axs-distribution.tar.gz

# Run a test
spark-submit \
>             --class org.apache.spark.examples.SparkPi \
>             --master yarn \
>             --deploy-mode cluster \
>             --driver-memory 1g \
>             --executor-memory 1g \
>             --executor-cores 1 \
>             examples/jars/spark-examples*.jar \
>                 10
20/12/11 18:55:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Exception in thread "main" java.lang.NoSuchMethodError: org.apache.hadoop.io.retry.RetryPolicies.retryOtherThanRemoteException(Lorg/apache/hadoop/io/retry/RetryPolicy;Ljava/util/Map;)Lorg/apache/hadoop/io/retry/RetryPolicy;
	at org.apache.hadoop.yarn.client.RMProxy.createRetryPolicy(RMProxy.java:255)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:91)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:72)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:187)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
	at org.apache.spark.deploy.yarn.Client.run(Client.scala:1134)
	at org.apache.spark.deploy.yarn.YarnClusterApplication.start(Client.scala:1526)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:849)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:167)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:195)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:86)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:924)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:935)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)




# Try building from source code
# fedora@master01
# Described here: https://github.com/astronomy-commons/axs/issues/20
# --------------------------------------------------------------------


# Clone axs repo
git clone https://github.com/astronomy-commons/axs.git
pushd axs
  git checkout master
popd

# Clone the axs-spark repository

git clone https://github.com/astronomy-commons/axs-spark
pushd axs-spark
  git checkout axs-3.0.0-preview
popd


# Build AxsUtilities.jar
pushd axs/AxsUtilities
  mvn package 
popd

# Merge axs and Spark
cp -r ./axs/axs ./axs-spark/python/. # adds python components of axs to Spark's PYTHONPATH
cp -r ./axs/AxsUtilities/target/*.jar ./axs-spark/python/axs/. # adds compiled  AXS Jar for use in Spark

# Build Spark from source
pushd axs-spark 
  ./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-2.7 -Pmesos -Pyarn -Phive -Phive-thriftserver 



+++ dirname ./dev/make-distribution.sh
++ cd ./dev/..
++ pwd
+ SPARK_HOME=/home/fedora/axs-spark
+ DISTDIR=/home/fedora/axs-spark/dist
+ MAKE_TGZ=false
+ MAKE_PIP=false
+ MAKE_R=false
+ NAME=none
+ MVN=/home/fedora/axs-spark/build/mvn
+ ((  8  ))
+ case $1 in
+ NAME=AXS-Custom-Build
+ shift
+ shift
+ ((  6  ))
+ case $1 in
+ MAKE_TGZ=true
+ shift
+ ((  5  ))
+ case $1 in
+ break
+ '[' -z /etc/alternatives/jre ']'
+ '[' -z /etc/alternatives/jre ']'
++ command -v git
+ '[' /usr/bin/git ']'
++ git rev-parse --short HEAD
+ GITREV=c03be446ed
+ '[' '!' -z c03be446ed ']'
+ GITREVSTRING=' (git revision c03be446ed)'
+ unset GITREV
++ command -v /home/fedora/axs-spark/build/mvn
+ '[' '!' /home/fedora/axs-spark/build/mvn ']'
++ /home/fedora/axs-spark/build/mvn help:evaluate -Dexpression=project.version -Phadoop-2.7 -Pmesos -Pyarn -Phive -Phive-thriftserver
++ grep -v INFO
++ grep -v WARNING
++ tail -n 1
+ VERSION=


popd


# Nothing in dist/ ??


# Try using their latest build with the right version of Hadoop
# Described here: https://github.com/astronomy-commons/axs/issues/20

# Modify Ansible scripts, and replace Hadoop and Spark version everywhere
# fedora@master01
# --------------------------------------------------------------------

https://archive.apache.org/dist/hadoop/core/hadoop-2.7.4/hadoop-2.7.4.tar.gz
https://github.com/stevenstetzler/axs/releases/download/v3.0.0-preview/axs-distribution.tgz





# Start dfs
# fedora@master01
# -------------------------------

start-dfs.sh

# Check dfsadmin report

hdfs dfsadmin -report
Safe mode is ON
Configured Capacity: 0 (0 B)
Present Capacity: 0 (0 B)
DFS Remaining: 0 (0 B)
DFS Used: 0 (0 B)
DFS Used%: NaN%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0

-------------------------------------------------

# Check Worker node logs..
# fedora@worker01
# ------------------------------------

2020-12-13 16:11:11,641 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-12-13 16:11:12,021 INFO org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker: Scheduling a check for [DISK]file:/tmp/hadoop-fedora/dfs/data
2020-12-13 16:11:12,043 WARN org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker: Exception checking StorageLocation [DISK]file:/tmp/hadoop-fedora/dfs/data
java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Ljava/lang/String;)Lorg/apache/hadoop/io/nativeio/NativeIO$POSIX$Stat;
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.stat(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO$POSIX.getStat(NativeIO.java:451)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfoByNativeIO(RawLocalFileSystem.java:796)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.loadPermissionInfo(RawLocalFileSystem.java:710)
	at org.apache.hadoop.fs.RawLocalFileSystem$DeprecatedRawLocalFileStatus.getPermission(RawLocalFileSystem.java:678)
	at org.apache.hadoop.util.DiskChecker.mkdirsWithExistsAndPermissionCheck(DiskChecker.java:233)
	at org.apache.hadoop.util.DiskChecker.checkDirInternal(DiskChecker.java:141)
	at org.apache.hadoop.util.DiskChecker.checkDir(DiskChecker.java:116)
	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:239)
	at org.apache.hadoop.hdfs.server.datanode.StorageLocation.check(StorageLocation.java:52)
	at org.apache.hadoop.hdfs.server.datanode.checker.ThrottledAsyncChecker$1.call(ThrottledAsyncChecker.java:142)
	at com.google.common.util.concurrent.TrustedListenableFutureTask$TrustedFutureInterruptibleTask.runInterruptibly(TrustedListenableFutureTask.java:125)
	at com.google.common.util.concurrent.InterruptibleTask.run(InterruptibleTask.java:57)
	at com.google.common.util.concurrent.TrustedListenableFutureTask.run(TrustedListenableFutureTask.java:78)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)
2020-12-13 16:11:12,045 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: Exception in secureMain
org.apache.hadoop.util.DiskChecker$DiskErrorException: Too many failed volumes - current valid volumes: 0, volumes configured: 1, volumes failed: 1, volume failures tolerated: 0
	at org.apache.hadoop.hdfs.server.datanode.checker.StorageLocationChecker.check(StorageLocationChecker.java:231)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.makeInstance(DataNode.java:2799)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:2714)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.createDataNode(DataNode.java:2756)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.secureMain(DataNode.java:2900)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.main(DataNode.java:2924)
2020-12-13 16:11:12,046 INFO org.apache.hadoop.util.ExitUtil: Exiting with status 1: org.apache.hadoop.util.DiskChecker$DiskErrorException: Too many failed volumes - current valid volumes: 0, volumes configured: 1, volumes failed: 1, volume failures tolerated: 0
2020-12-13 16:11:12,048 INFO org.apache.hadoop.hdfs.server.datanode.DataNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down DataNode at worker01/10.10.3.28
************************************************************/



# Master node logs..
# fedora@master01
# ------------------------------------



2020-12-13 16:38:45,116 ERROR org.apache.hadoop.hdfs.server.namenode.FSEditLog: Error: finalize log segment 5, 6 failed for (journal JournalAndStream(mgr=FileJournalManager(root=/var/local/hadoop/namenode/fsimage), stream=null))
java.lang.IllegalStateException: Unable to finalize edits file /var/local/hadoop/namenode/fsimage/current/edits_inprogress_0000000000000000005
	at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.finalizeLogSegment(FileJournalManager.java:143)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet$4.apply(JournalSet.java:236)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.mapJournalsAndReportErrors(JournalSet.java:393)
	at org.apache.hadoop.hdfs.server.namenode.JournalSet.finalizeLogSegment(JournalSet.java:231)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.endCurrentLogSegment(FSEditLog.java:1325)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.rollEditLog(FSEditLog.java:1254)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.rollEditLog(FSImage.java:1347)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.rollEditLog(FSNamesystem.java:5807)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollEditLog(NameNodeRpcServer.java:1122)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolServerSideTranslatorPB.rollEditLog(NamenodeProtocolServerSideTranslatorPB.java:142)
	at org.apache.hadoop.hdfs.protocol.proto.NamenodeProtocolProtos$NamenodeProtocolService$2.callBlockingMethod(NamenodeProtocolProtos.java:12025)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:616)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:982)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2217)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:2213)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1746)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2213)
Caused by: ENOENT: No such file or directory
	at org.apache.hadoop.io.nativeio.NativeIO.renameTo0(Native Method)
	at org.apache.hadoop.io.nativeio.NativeIO.renameTo(NativeIO.java:880)
	at org.apache.hadoop.hdfs.server.namenode.FileJournalManager.finalizeLogSegment(FileJournalManager.java:140)
	... 18 more



# Copy Hadoop configurations from previous Hadoop (3.1.3) into new (2.7.4) and restart dfs
# fedora@master01
# ------------------------------------

stop-dfs.sh
start.dfs.sh


# Check dfs report
# ------------------------------------
# fedora@master01

hdfs dfsadmin -report
Configured Capacity: 2748779069440 (2.50 TB)
Present Capacity: 2737951252480 (2.49 TB)
DFS Remaining: 2737951211520 (2.49 TB)
DFS Used: 40960 (40 KB)
DFS Used%: 0.00%
Under replicated blocks: 0
Blocks with corrupt replicas: 0
Missing blocks: 0
Missing blocks (with replication factor 1): 0

-------------------------------------------------
Live datanodes (5):

Name: 10.10.1.122:50010 (worker01)
Hostname: worker01
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 17293312 (16.49 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Dec 14 15:08:29 UTC 2020


Name: 10.10.3.203:50010 (worker06)
Hostname: worker06
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 17293312 (16.49 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Dec 14 15:08:29 UTC 2020


Name: 10.10.3.18:50010 (worker05)
Hostname: worker05
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 17293312 (16.49 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Dec 14 15:08:29 UTC 2020


Name: 10.10.0.247:50010 (worker04)
Hostname: worker04
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 17293312 (16.49 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Dec 14 15:08:30 UTC 2020


Name: 10.10.0.141:50010 (worker02)
Hostname: worker02
Decommission Status : Normal
Configured Capacity: 549755813888 (512 GB)
DFS Used: 8192 (8 KB)
Non DFS Used: 17293312 (16.49 MB)
DFS Remaining: 547590242304 (509.98 GB)
DFS Used%: 0.00%
DFS Remaining%: 99.61%
Configured Cache Capacity: 0 (0 B)
Cache Used: 0 (0 B)
Cache Remaining: 0 (0 B)
Cache Used%: 100.00%
Cache Remaining%: 0.00%
Xceivers: 1
Last contact: Mon Dec 14 15:08:29 UTC 2020


# Try a spark job:
# fedora@master01
# ------------------------------------

bin/spark-submit             --class org.apache.spark.examples.SparkPi             --master yarn             --deploy-mode cluster             --driver-memory 1g             --executor-memory 1g             --executor-cores 1             examples/jars/spark-examples*.jar                 10

-bash: spark-submit: command not found

# spark-submit not found, something wrong with paths it seems

# Try with full path
# fedora@master01
/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/bin/spark-submit             --class org.apache.spark.examples.SparkPi             --master yarn             --deploy-mode cluster             --driver-memory 1g             --executor-memory 1g             --executor-cores 1             examples/jars/spark-examples*.jar                 10


20/12/14 15:13:44 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.106:8032
20/12/14 15:13:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/14 15:13:44 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers
20/12/14 15:13:44 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (20000 MB per container)
20/12/14 15:13:44 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
20/12/14 15:13:44 INFO yarn.Client: Setting up container launch context for our AM
20/12/14 15:13:44 INFO yarn.Client: Setting up the launch environment for our AM container
20/12/14 15:13:44 INFO yarn.Client: Preparing resources for our AM container
20/12/14 15:13:44 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/14 15:13:46 INFO yarn.Client: Uploading resource file:/tmp/spark-b09c97d1-dd53-4d98-bcc4-9ded60b402b6/__spark_libs__6199230649330192539.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1607881005213_0002/__spark_libs__6199230649330192539.zip
20/12/14 15:14:46 INFO hdfs.DFSClient: Exception in createBlockOutputStream
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=worker06/10.10.3.203:50010]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1702)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1432)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1385)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)
20/12/14 15:14:46 INFO hdfs.DFSClient: Abandoning BP-2057797429-10.10.0.106-1607879974746:blk_1073741829_1005
20/12/14 15:14:46 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.10.3.203:50010,DS-41991e17-9dfb-4ca2-98be-723c0d583f43,DISK]
20/12/14 15:15:46 INFO hdfs.DFSClient: Exception in createBlockOutputStream
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=worker01/10.10.1.122:50010]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1702)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1432)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1385)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)
20/12/14 15:15:46 INFO hdfs.DFSClient: Abandoning BP-2057797429-10.10.0.106-1607879974746:blk_1073741830_1006
20/12/14 15:15:46 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.10.1.122:50010,DS-8438141a-e12a-436c-a775-e2a81d86adac,DISK]


# Check worker node logs
# fedora@worker01
# ------------------------------------



ls -al /var/local/hadoop/logs/
total 40
drwxrwsr-x. 2 fedora fedora  4096 Dec 13 17:36 .
drwxrwsr-x. 3 fedora fedora  4096 Dec 13 17:16 ..
-rw-rw-r--. 1 fedora fedora     0 Dec 13 17:36 SecurityAuth-fedora.audit
-rw-rw-r--. 1 fedora fedora 25436 Dec 14 12:35 hadoop-fedora-datanode-aglais-20201213-worker01.novalocal.log
-rw-rw-r--. 1 fedora fedora   726 Dec 13 17:36 hadoop-fedora-datanode-aglais-20201213-worker01.novalocal.out


# Only datanode logs? 
# Start Yarn 
start-yarn.sh 
>
	starting yarn daemons
	starting resourcemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-resourcemanager-aglais-20201213-master01.novalocal.out
	worker01: mkdir: cannot create directory ‘/opt/hadoop-2.7.4/logs’: Permission denied
	worker01: chown: cannot access '/opt/hadoop-2.7.4/logs': No such file or directory
	worker01: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out
	worker01: /opt/hadoop/sbin/yarn-daemon.sh: line 124: /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out: No such file or directory
	worker05: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker05.novalocal.out
	worker06: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker06.novalocal.out
	worker02: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker02.novalocal.out
	worker04: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker04.novalocal.out
	worker03: fedora@worker03: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
	worker01: head: cannot open '/opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out' for reading: No such file or directory
	worker01: /opt/hadoop/sbin/yarn-daemon.sh: line 129: /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out: No such file or directory
	worker01: /opt/hadoop/sbin/yarn-daemon.sh: line 130: /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out: No such file or directory

# Exceptions on worker01 and worker03 


# Check Hadoop/Yarn GUI on local:
# user@local
# -------------------------------
# ssh -L '8088:master01:8088' fedora@128.232.227.229 

# 4 nodes show up as available, all except worker01 and worker03


# Try a job:
# fedora@master01
# -------------------------------
/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/bin/spark-submit             --class org.apache.spark.examples.SparkPi             --master yarn             --deploy-mode cluster             --driver-memory 1g             --executor-memory 1g             --executor-cores 1             examples/jars/spark-examples*.jar                 10
20/12/14 15:59:30 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.106:8032
20/12/14 15:59:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
20/12/14 15:59:30 INFO yarn.Client: Requesting a new application from cluster with 4 NodeManagers
20/12/14 15:59:30 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (20000 MB per container)
20/12/14 15:59:30 INFO yarn.Client: Will allocate AM container, with 1408 MB memory including 384 MB overhead
20/12/14 15:59:30 INFO yarn.Client: Setting up container launch context for our AM
20/12/14 15:59:30 INFO yarn.Client: Setting up the launch environment for our AM container
20/12/14 15:59:30 INFO yarn.Client: Preparing resources for our AM container
20/12/14 15:59:30 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
20/12/14 15:59:32 INFO yarn.Client: Uploading resource file:/tmp/spark-b26c582b-cbd7-4343-814e-5ac438713778/__spark_libs__7159098285007326622.zip -> hdfs://master01:9000/user/fedora/.sparkStaging/application_1607960056000_0002/__spark_libs__7159098285007326622.zip
	20/12/14 16:00:32 INFO hdfs.DFSClient: Exception in createBlockOutputStream
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=worker05/10.10.3.18:50010]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1702)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1432)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1385)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)
20/12/14 16:00:32 INFO hdfs.DFSClient: Abandoning BP-2057797429-10.10.0.106-1607879974746:blk_1073741837_1013
20/12/14 16:00:32 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.10.3.18:50010,DS-726a0278-56fe-44f1-82f6-c112e838f1a0,DISK]
20/12/14 16:01:32 INFO hdfs.DFSClient: Exception in createBlockOutputStream
org.apache.hadoop.net.ConnectTimeoutException: 60000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=worker04/10.10.0.247:50010]
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:534)
	at org.apache.hadoop.hdfs.DFSOutputStream.createSocketForPipeline(DFSOutputStream.java:1702)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.createBlockOutputStream(DFSOutputStream.java:1432)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.nextBlockOutputStream(DFSOutputStream.java:1385)
	at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:554)
20/12/14 16:01:32 INFO hdfs.DFSClient: Abandoning BP-2057797429-10.10.0.106-1607879974746:blk_1073741838_1014
20/12/14 16:01:32 INFO hdfs.DFSClient: Excluding datanode DatanodeInfoWithStorage[10.10.0.247:50010,DS-ebe260ed-87ec-4623-a61c-d772e4eb93ee,DISK]


# Check if Yarn is running
# fedora@master01
# --------------------------


yarn application -list
20/12/14 16:35:09 INFO client.RMProxy: Connecting to ResourceManager at master01/10.10.0.106:8032
Total number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):0
                Application-Id	    Application-Name	    Application-Type	      User	     Queue	             State	       Final-State	       Progress	                       Tracking-URL


# Check that HDFS is working:
# fedora@master01
# --------------------------

hdfs dfs -mkdir /test
hdfs dfs -ls /
Found 3 items
drwxr-xr-x   - fedora supergroup          0 2020-12-13 17:19 /spark-log
drwxr-xr-x   - fedora supergroup          0 2020-12-14 16:49 /test
drwxr-xr-x   - fedora supergroup          0 2020-12-13 17:37 /user


# Try this:
# https://stackoverflow.com/questions/40546515/mapreduce-jobs-failing-after-accepted-by-yarn
# fedora@master01
# ---------------------------------
for node in worker01 worker02 worker03 worker04 worker05 worker06; do
    scp /opt/hadoop-2.7.4/etc/hadoop/yarn-site.xml $node:/opt/hadoop-2.7.4/etc/hadoop/;
done

>
yarn-site.xml                                                                                                                                                              100% 3659     7.5MB/s   00:00    
yarn-site.xml                                                                                                                                                              100% 3659    10.1MB/s   00:00    
fedora@worker03: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
lost connection
yarn-site.xml                                                                                                                                                              100% 3659     8.2MB/s   00:00    
yarn-site.xml                                                                                                                                                              100% 3659     8.9MB/s   00:00    
yarn-site.xml   


# Why cant we log in to worker03??
# Ignore for now..


# Restart our Hadoop services
# fedora@master01
# ---------------------------------

stop-all.sh
start-all.sh


# Perhaps this Hadoop version requires different ports open?
# ...worker05/10.10.3.18:50010


# Open ALL ports on worker nodes (for experiment) and restart VMs and hadoop
# fedora@master01
# -----------------------
stop-all.sh
start-all.sh 
This script is Deprecated. Instead use start-dfs.sh and start-yarn.sh
Starting namenodes on [master01]
master01: starting namenode, logging to /var/local/hadoop/logs/hadoop-fedora-namenode-aglais-20201213-master01.novalocal.out
worker04: starting datanode, logging to /var/local/hadoop/logs/hadoop-fedora-datanode-aglais-20201213-worker04.novalocal.out
worker02: starting datanode, logging to /var/local/hadoop/logs/hadoop-fedora-datanode-aglais-20201213-worker02.novalocal.out
worker01: starting datanode, logging to /var/local/hadoop/logs/hadoop-fedora-datanode-aglais-20201213-worker01.novalocal.out
worker05: starting datanode, logging to /var/local/hadoop/logs/hadoop-fedora-datanode-aglais-20201213-worker05.novalocal.out
worker06: starting datanode, logging to /var/local/hadoop/logs/hadoop-fedora-datanode-aglais-20201213-worker06.novalocal.out
worker03: fedora@worker03: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).
Starting secondary namenodes [0.0.0.0]
0.0.0.0: starting secondarynamenode, logging to /var/local/hadoop/logs/hadoop-fedora-secondarynamenode-aglais-20201213-master01.novalocal.out
starting yarn daemons
starting resourcemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-resourcemanager-aglais-20201213-master01.novalocal.out
worker01: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker01.novalocal.out
worker04: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker04.novalocal.out
worker02: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker02.novalocal.out
worker06: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker06.novalocal.out
worker05: starting nodemanager, logging to /opt/hadoop-2.7.4/logs/yarn-fedora-nodemanager-aglais-20201213-worker05.novalocal.out
worker03: fedora@worker03: Permission denied (publickey,gssapi-keyex,gssapi-with-mic).



# Try the Spark example again..

20/12/14 17:49:36 INFO yarn.Client: Application report for application_1607968005905_0003 (state: RUNNING)
20/12/14 17:49:37 INFO yarn.Client: Application report for application_1607968005905_0003 (state: RUNNING)
20/12/14 17:49:38 INFO yarn.Client: Application report for application_1607968005905_0003 (state: RUNNING)
20/12/14 17:49:39 INFO yarn.Client: Application report for application_1607968005905_0003 (state: RUNNING)
20/12/14 17:49:40 INFO yarn.Client: Application report for application_1607968005905_0003 (state: FINISHED)
20/12/14 17:49:40 INFO yarn.Client: 
	 client token: N/A
	 diagnostics: N/A
	 ApplicationMaster host: worker02
	 ApplicationMaster RPC port: 44023
	 queue: default
	 start time: 1607968169749
	 final status: SUCCEEDED
	 tracking URL: http://master01:8088/proxy/application_1607968005905_0003/
	 user: fedora
20/12/14 17:49:40 INFO yarn.Client: Deleted staging directory hdfs://master01:9000/user/fedora/.sparkStaging/application_1607968005905_0003
20/12/14 17:49:40 INFO util.ShutdownHookManager: Shutdown hook called
20/12/14 17:49:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-4301d17e-314c-4f93-9a30-1d233b8eebea
20/12/14 17:49:40 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-0ed727e7-9156-402a-a523-bb3270714e95


# Success


# Let's check if/how we can validate that axs is installed
# fedora@master01
# --------------------------------------------------------------

# Check Pyspark
pyspark --master=local[10] --driver-memory=64g
-bash: python: command not found


# Looks like something went wrong during Ansible installation..
# Python is missing, and Zeppelin is missing from Zeppelin node..

# We need to run it the deploy again and see
# For now manually install python (sudo yum install python)

pyspark --master=local[10] --driver-memory=64g

>> from axs import AxsCatalog
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/python/axs/__init__.py", line 9, in <module>
    from axs.catalog import AxsCatalog
  File "/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/python/axs/catalog.py", line 2, in <module>
    from axs.axsframe import AxsFrame
  File "/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/python/axs/axsframe.py", line 3, in <module>
    import numpy as np
ImportError: No module named numpy

...

# Running this a few times, we see that there are a number of libraries that are required which are missing:
# Fix by installing with pip
# -------------------------------------------------------------------------

sudo pip install numpy pandas pyarrow


# Try again
# -----------------------

pyspark --master=local[10] --driver-memory=64g

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview
      /_/

Using Python version 2.7.18 (default, Apr 21 2020 18:49:31)
SparkSession available as 'spark'.
>>> from axs import AxsCatalog
>>> db = AxsCatalog(spark)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/python/axs/catalog.py", line 41, in __init__
    self._CatalogUtils = self.spark.sparkContext._jvm.org.dirac.axs.util.CatalogUtils()
TypeError: 'JavaPackage' object is not callable


# A new exception..
# https://github.com/JohnSnowLabs/spark-nlp/issues/232

# Check that the AXS jar exists under jars..

# ...

# Can't seem to find it.. Let's try fetching another version found in the github issue described above
# fedora@master01
# -----------------------------------------------

wget https://epyc.astro.washington.edu/~ctslater/axs-spark-3.0.0-preview-axsdistfix.tar.gz
tar -xzvf axs-spark-3.0.0-preview-axsdistfix.tar.gz
pushd spark-3.0.0-preview-bin-spark-axs-hive
ls jars/
..
 
 AxsUtilities-1.0-SNAPSHOT.jar .
..

# Let's copy it over and try creating an AXS catalog again

sudo cp jars/AxsUtilities-1.0-SNAPSHOT.jar . /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/jars/


bin/pyspark --jars jars/AxsUtilities-1.0-SNAPSHOT.jar 

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 3.0.0-preview
      /_/

Using Python version 2.7.18 (default, Apr 21 2020 18:49:31)
SparkSession available as 'spark'.
>>> from axs import AxsCatalog
>>> db = AxsCatalog(spark)


# Success?



# Let's now try installing Zeppelin again
# Run Ansible scripts that install Zeppelin
# user@ansible
# -----------------------------------------------

pushd /hadoop-yarn/ansible

   ansible-playbook         --inventory "hosts.yml"         "combined-04.yml"

PLAY RECAP **************************************************************************************************************************************************************************************************
localhost                  : ok=3    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=7    changed=7    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=13   changed=12   unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   


popd


# Setup user accounts, setup hadoop directory in Zeppelin and start Zeppelin
# user@zeppelins
# -----------------------------------------------

ssh zeppelin
# Setup accounts in conf/shiro.ini
# Copy over Hadoop directory from master node

/home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh start
Log dir doesn't exist, create /home/fedora/zeppelin-0.8.2-bin-all/logs
Pid dir doesn't exist, create /home/fedora/zeppelin-0.8.2-bin-all/run
Zeppelin start                                             [  OK  ]


# Check Zeppelin in GUI 
http://128.232.227.229:8080/#/notebook/2FT3K6MGA


Exception in thread "main" org.apache.spark.SparkException: Master must either be yarn or start with spark, mesos, k8s, or local
	at org.apache.spark.deploy.SparkSubmit.error(SparkSubmit.scala:936)
	at org.apache.spark.deploy.SparkSubmit.prepareSubmitEnvironment(SparkSubmit.scala:238)
	at org.apache.spark.deploy.SparkSubmit.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:871)
	at org.apache.spark.deploy.SparkSubmit.doRunMain$1(SparkSubmit.scala:180)
	at org.apache.spark.deploy.SparkSubmit.submit(SparkSubmit.scala:203)
	at org.apache.spark.deploy.SparkSubmit.doSubmit(SparkSubmit.scala:90)
	at org.apache.spark.deploy.SparkSubmit$$anon$2.doSubmit(SparkSubmit.scala:1007)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:1016)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)

# If we set master = yarn	

java.lang.RuntimeException: master is set as yarn, but spark.submit.deployMode is not specified
	at org.apache.zeppelin.interpreter.launcher.SparkInterpreterLauncher.getDeployMode(SparkInterpreterLauncher.java:206)
	at org.apache.zeppelin.interpreter.launcher.SparkInterpreterLauncher.buildEnvFromProperties(SparkInterpreterLauncher.java:59)
	at org.apache.zeppelin.interpreter.launcher.ShellScriptLauncher.launch(ShellScriptLauncher.java:87)
	at org.apache.zeppelin.interpreter.InterpreterSetting.createInterpreterProcess(InterpreterSetting.java:715)
	at org.apache.zeppelin.interpreter.ManagedInterpreterGroup.getOrCreateInterpreterProcess(ManagedInterpreterGroup.java:62)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getOrCreateInterpreterProcess(RemoteInterpreter.java:111)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.internal_create(RemoteInterpreter.java:164)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.open(RemoteInterpreter.java:132)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreter.getFormType(RemoteInterpreter.java:299)
	at org.apache.zeppelin.notebook.Paragraph.jobRun(Paragraph.java:408)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.RemoteScheduler$JobRunner.run(RemoteScheduler.java:315)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)

# If we then set spark.submit.deployMode = client


java.lang.NoSuchMethodError: scala.Predef$.refArrayOps([Ljava/lang/Object;)Lscala/collection/mutable/ArrayOps;
	at org.apache.zeppelin.spark.BaseSparkScalaInterpreter.getUserJars(BaseSparkScalaInterpreter.scala:382)
	at org.apache.zeppelin.spark.SparkScala211Interpreter.open(SparkScala211Interpreter.scala:71)
	at org.apache.zeppelin.spark.NewSparkInterpreter.open(NewSparkInterpreter.java:102)
	at org.apache.zeppelin.spark.SparkInterpreter.open(SparkInterpreter.java:62)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
	at org.apache.zeppelin.spark.IPySparkInterpreter.getSparkInterpreter(IPySparkInterpreter.java:94)
	at org.apache.zeppelin.spark.IPySparkInterpreter.open(IPySparkInterpreter.java:54)
	at org.apache.zeppelin.spark.PySparkInterpreter.open(PySparkInterpreter.java:129)
	at org.apache.zeppelin.interpreter.LazyOpenInterpreter.open(LazyOpenInterpreter.java:69)
	at org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:616)
	at org.apache.zeppelin.scheduler.Job.run(Job.java:188)
	at org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)
	at java.util.concurrent.FutureTask.run(FutureTask.java:266)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)


# .....???
