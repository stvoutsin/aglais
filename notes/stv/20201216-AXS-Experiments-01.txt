#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

Target:
 
  Create custom distribution of Axs-spark and deploy on our cluster


# Install maven
# fedora@master01
# ----------------------------------
sudo yum install maven



# Clone and checkout AXS repos
# fedora@master01
# ----------------------------------
git clone https://github.com/astronomy-commons/axs.git
pushd axs
  git checkout master
popd


# Next do the same with the axs-spark repository
git clone https://github.com/astronomy-commons/axs-spark
pushd axs-spark
    git checkout axs-3.0.0-preview
popd 


# Build AxsUtilities.jar
# fedora@master01
# ----------------------------------
pushd axs/AxsUtilities
  mvn package # runs maven to compile the AxsUtilities project, pom.xml sets configuration for build
popd



# Merge axs and Spark
# fedora@master01
# ----------------------------------

cp -r ./axs/axs ./axs-spark/python/. # adds python components of axs to Spark's PYTHONPATH
cp -r ./axs/AxsUtilities/target/*.jar ./axs-spark/python/axs/. # adds compiled  AXS Jar for use in Spark


# Build Spark from source
# fedora@master01
# ----------------------------------

cd axs-spark
./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-2.7.4 -Pmesos -Pyarn -Phive -Phive-thriftserver -Pkubernetes

..


[ERROR] ## Exception when compiling 10 sources to /home/fedora/axs-spark/common/tags/target/scala-2.12/classes
java.io.IOException: Cannot run program "/etc/alternatives/jre/bin/javac" (in directory "/home/fedora/axs-spark"): error=2, No such file or directory
java.lang.ProcessBuilder.start(ProcessBuilder.java:1048)
scala.sys.process.ProcessBuilderImpl$Simple.run(ProcessBuilderImpl.scala:75)
scala.sys.process.ProcessBuilderImpl$AbstractBuilder.run(ProcessBuilderImpl.scala:106)
scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$anonfun$runBuffered$1(ProcessBuilderImpl.scala:154)
scala.runtime.java8.JFunction0$mcI$sp.apply(JFunction0$mcI$sp.java:23)
sbt.internal.inc.javac.JavacLogger.buffer(JavacProcessLogger.scala:49)
scala.sys.process.ProcessBuilderImpl$AbstractBuilder.runBuffered(ProcessBuilderImpl.scala:154)
scala.sys.process.ProcessBuilderImpl$AbstractBuilder.$bang(ProcessBuilderImpl.scala:120)
sbt.internal.inc.javac.ForkedJava$.$anonfun$launch$3(ForkedJava.scala:50)
sbt.internal.inc.javac.ForkedJava$.$anonfun$launch$3$adapted(ForkedJava.scala:43)
sbt.internal.inc.javac.ForkedJava$.$anonfun$withArgumentFile$1(ForkedJava.scala:72)
sbt.io.IO$.withTemporaryDirectory(IO.scala:489)
sbt.io.IO$.withTemporaryDirectory(IO.scala:499)
sbt.internal.inc.javac.ForkedJava$.withArgumentFile(ForkedJava.scala:69)
sbt.internal.inc.javac.ForkedJava$.launch(ForkedJava.scala:43)
sbt.internal.inc.javac.ForkedJavaCompiler.run(ForkedJava.scala:98)
sbt.internal.inc.javac.AnalyzingJavaCompiler.$anonfun$compile$11(AnalyzingJavaCompiler.scala:158)
scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
sbt.internal.inc.javac.AnalyzingJavaCompiler.timed(AnalyzingJavaCompiler.scala:236)
sbt.internal.inc.javac.AnalyzingJavaCompiler.compile(AnalyzingJavaCompiler.scala:148)
sbt.internal.inc.MixedAnalyzingCompiler.$anonfun$compile$5(MixedAnalyzingCompiler.scala:134)
scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)
sbt.internal.inc.MixedAnalyzingCompiler.timed(MixedAnalyzingCompiler.scala:186)
sbt.internal.inc.MixedAnalyzingCompiler.compileJava$1(MixedAnalyzingCompiler.scala:100)
sbt.internal.inc.MixedAnalyzingCompiler.compile(MixedAnalyzingCompiler.scala:146)
sbt.internal.inc.IncrementalCompilerImpl.$anonfun$compileInternal$1(IncrementalCompilerImpl.scala:343)
sbt.internal.inc.IncrementalCompilerImpl.$anonfun$compileInternal$1$adapted(IncrementalCompilerImpl.scala:343)
sbt.internal.inc.Incremental$.doCompile(Incremental.scala:120)
sbt.internal.inc.Incremental$.$anonfun$compile$4(Incremental.scala:100)
sbt.internal.inc.IncrementalCommon.recompileClasses(IncrementalCommon.scala:180)
sbt.internal.inc.IncrementalCommon.cycle(IncrementalCommon.scala:98)
sbt.internal.inc.Incremental$.$anonfun$compile$3(Incremental.scala:102)
sbt.internal.inc.Incremental$.manageClassfiles(Incremental.scala:155)
sbt.internal.inc.Incremental$.compile(Incremental.scala:92)
sbt.internal.inc.IncrementalCompile$.apply(Compile.scala:75)
sbt.internal.inc.IncrementalCompilerImpl.compileInternal(IncrementalCompilerImpl.scala:348)
sbt.internal.inc.IncrementalCompilerImpl.$anonfun$compileIncrementally$1(IncrementalCompilerImpl.scala:301)
sbt.internal.inc.IncrementalCompilerImpl.handleCompilationError(IncrementalCompilerImpl.scala:168)
sbt.internal.inc.IncrementalCompilerImpl.compileIncrementally(IncrementalCompilerImpl.scala:248)
sbt.internal.inc.IncrementalCompilerImpl.compile(IncrementalCompilerImpl.scala:74)
sbt_inc.SbtIncrementalCompiler.compile(SbtIncrementalCompiler.java:172)
scala_maven.ScalaCompilerSupport.incrementalCompile(ScalaCompilerSupport.java:291)
scala_maven.ScalaCompilerSupport.compile(ScalaCompilerSupport.java:110)
scala_maven.ScalaCompilerSupport.doExecute(ScalaCompilerSupport.java:92)
scala_maven.ScalaMojoSupport.execute(ScalaMojoSupport.java:557)
org.apache.maven.plugin.DefaultBuildPluginManager.executeMojo(DefaultBuildPluginManager.java:137)
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:210)
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:156)
org.apache.maven.lifecycle.internal.MojoExecutor.execute(MojoExecutor.java:148)
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:117)
org.apache.maven.lifecycle.internal.LifecycleModuleBuilder.buildProject(LifecycleModuleBuilder.java:81)
org.apache.maven.lifecycle.internal.builder.singlethreaded.SingleThreadedBuilder.build(SingleThreadedBuilder.java:56)
org.apache.maven.lifecycle.internal.LifecycleStarter.execute(LifecycleStarter.java:128)
org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:305)
org.apache.maven.DefaultMaven.doExecute(DefaultMaven.java:192)
org.apache.maven.DefaultMaven.execute(DefaultMaven.java:105)
org.apache.maven.cli.MavenCli.execute(MavenCli.java:957)
org.apache.maven.cli.MavenCli.doMain(MavenCli.java:289)
org.apache.maven.cli.MavenCli.main(MavenCli.java:193)
sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
java.lang.reflect.Method.invoke(Method.java:498)
org.codehaus.plexus.classworlds.launcher.Launcher.launchEnhanced(Launcher.java:282)
org.codehaus.plexus.classworlds.launcher.Launcher.launch(Launcher.java:225)
org.codehaus.plexus.classworlds.launcher.Launcher.mainWithExitCode(Launcher.java:406)
org.codehaus.plexus.classworlds.launcher.Launcher.main(Launcher.java:347)
           
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary for Spark Project Parent POM 3.1.0-SNAPSHOT:
[INFO] 
[INFO] Spark Project Parent POM ........................... SUCCESS [  1.977 s]
[INFO] Spark Project Tags ................................. FAILURE [  3.616 s]
[INFO] Spark Project Sketch ............................... SKIPPED
[INFO] Spark Project Local DB ............................. SKIPPED
[INFO] Spark Project Networking ........................... SKIPPED
[INFO] Spark Project Shuffle Streaming Service ............ SKIPPED
[INFO] Spark Project Unsafe ............................... SKIPPED
[INFO] Spark Project Launcher ............................. SKIPPED
[INFO] Spark Project Core ................................. SKIPPED
[INFO] Spark Project ML Local Library ..................... SKIPPED
[INFO] Spark Project GraphX ............................... SKIPPED
[INFO] Spark Project Streaming ............................ SKIPPED
[INFO] Spark Project Catalyst ............................. SKIPPED
[INFO] Spark Project SQL .................................. SKIPPED
[INFO] Spark Project ML Library ........................... SKIPPED
[INFO] Spark Project Tools ................................ SKIPPED
[INFO] Spark Project Hive ................................. SKIPPED
[INFO] Spark Project REPL ................................. SKIPPED
[INFO] Spark Project Assembly ............................. SKIPPED
[INFO] Kafka 0.10+ Token Provider for Streaming ........... SKIPPED
[INFO] Spark Integration for Kafka 0.10 ................... SKIPPED
[INFO] Kafka 0.10+ Source for Structured Streaming ........ SKIPPED
[INFO] Spark Project Examples ............................. SKIPPED
[INFO] Spark Integration for Kafka 0.10 Assembly .......... SKIPPED
[INFO] Spark Avro ......................................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time:  6.290 s
[INFO] Finished at: 2020-12-16T13:09:57Z
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal net.alchim31.maven:scala-maven-plugin:4.4.0:compile (scala-compile-first) on project spark-tags_2.12: wrap: java.io.IOException: Cannot run program "/etc/alternatives/jre/bin/javac" (in directory "/home/fedora/axs-spark"): error=2, No such file or directory -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <args> -rf :spark-tags_2.12


# Check what is in the /etc/alternatives/jre directory
ls -al /etc/alternatives/jre
lrwxrwxrwx. 1 root root 63 Dec 16 13:05 /etc/alternatives/jre -> /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-0.fc30.x86_64/jre


# Change JAVA_HOME
export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.252.b09-0.fc30.x86_64/bin/


# Run make distribution again

./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-2.7.4 -Pmesos -Pyarn -Phive -Phive-thriftserver -Pkubernetes

..


Skipping building R source package
+ mkdir /home/fedora/axs-spark/dist/conf
+ cp /home/fedora/axs-spark/conf/fairscheduler.xml.template /home/fedora/axs-spark/conf/log4j.properties.template /home/fedora/axs-spark/conf/metrics.properties.template /home/fedora/axs-spark/conf/spark-defaults.conf.template /home/fedora/axs-spark/conf/spark-env.sh.template /home/fedora/axs-spark/conf/workers.template /home/fedora/axs-spark/dist/conf
+ cp /home/fedora/axs-spark/README.md /home/fedora/axs-spark/dist
+ cp -r /home/fedora/axs-spark/bin /home/fedora/axs-spark/dist
+ cp -r /home/fedora/axs-spark/python /home/fedora/axs-spark/dist
+ '[' false == true ']'
+ cp -r /home/fedora/axs-spark/sbin /home/fedora/axs-spark/dist
+ '[' -d /home/fedora/axs-spark/R/lib/SparkR ']'
+ '[' true == true ']'
+ TARDIR_NAME=spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build
+ TARDIR=/home/fedora/axs-spark/spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build
+ rm -rf /home/fedora/axs-spark/spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build
+ cp -r /home/fedora/axs-spark/dist /home/fedora/axs-spark/spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build
+ tar czf spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build.tgz -C /home/fedora/axs-spark spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build
+ rm -rf /home/fedora/axs-spark/spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build



ls
CONTRIBUTING.md  NOTICE         README.md     bin     common  data  docs      graphx        licenses         mllib-local  python             sbin                                           sql        tools
LICENSE          NOTICE-binary  appveyor.yml  binder  conf    dev   examples  hadoop-cloud  licenses-binary  pom.xml      repl               scalastyle-config.xml                          streaming
LICENSE-binary   R              assembly      build   core    dist  external  launcher      mllib            project      resource-managers  spark-3.1.0-SNAPSHOT-bin-AXS-Custom-Build.tgz  target


ls -al dist/
total 156
drwxrwxr-x. 12 fedora fedora  4096 Dec 16 13:54 .
drwxrwxr-x. 34 fedora fedora  4096 Dec 16 13:54 ..
-rw-rw-r--.  1 fedora fedora 23221 Dec 16 13:54 LICENSE
-rw-rw-r--.  1 fedora fedora 57677 Dec 16 13:54 NOTICE
-rw-rw-r--.  1 fedora fedora  4488 Dec 16 13:54 README.md
-rw-rw-r--.  1 fedora fedora   153 Dec 16 13:54 RELEASE
drwxrwxr-x.  2 fedora fedora  4096 Dec 16 13:54 bin
drwxrwxr-x.  2 fedora fedora  4096 Dec 16 13:54 conf
drwxrwxr-x.  5 fedora fedora  4096 Dec 16 13:54 data
drwxrwxr-x.  4 fedora fedora  4096 Dec 16 13:54 examples
drwxrwxr-x.  2 fedora fedora 16384 Dec 16 13:54 jars
drwxrwxr-x.  4 fedora fedora  4096 Dec 16 13:54 kubernetes
drwxrwxr-x.  2 fedora fedora  4096 Dec 16 13:54 licenses
drwxrwxr-x.  8 fedora fedora  4096 Dec 16 13:54 python
drwxrwxr-x.  2 fedora fedora  4096 Dec 16 13:54 sbin
drwxrwxr-x.  2 fedora fedora  4096 Dec 16 13:54 yarn


# This seems to have now worked. What about if we now try to change build for hadoop 3.1.3
./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-3.1.3 -Pmesos -Pyarn -Phive -Phive-thriftserver -Pkubernetes


# Success..


# Scala version is 2.12, which seems to be incompatible with Zeppelin

# Let's try and build an odler version of axs-spark
# Checkout branch -> axs-2.4.3
..

# Build again with 2.4.7, as this is the version setup currently on our test system.. 

./dev/make-distribution.sh --name AXS-Custom-Build --tgz -Phadoop-2.4.7 -Pmesos -Pyarn -Phive -Phive-thriftserver -Pkubernetes


# Try running a Spark cell
# user@local
# ----------------------------------------------------

%spark.pyspark
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)

---------------------------------------------------------------------------
ModuleNotFoundError                       Traceback (most recent call last)
<ipython-input-4-21281c5cef7d> in <module>
----> 1 from axs import AxsCatalog, Constants
      2 db = AxsCatalog(spark)

ModuleNotFoundError: No module named 'axs'


# Check that we can import axs in a pyspark shell
# fedora@master01
# ----------------------------------------------------
./pyspark


>>> import axs
>>> exit()

# Seems fine

# Let's try changing Spark deployMode to 'client':

# In Zeppelin / Spark Interpreter, set:
spark.submit.deployMode	 client

# Now we restart Zeppelin and try again..

# NOTE: Setting deployMode to client works, but cluster mode does not

# Success (Cell run without any errors)



# Try loading Gaia into AXS
# fedora@master01
# -------------------------------------------------

%spark.pyspark

db.import_existing_table('gaia', 'file:///data/gaia/dr2', num_buckets=500, zone_height=Constants.ONE_AMIN,
    import_into_spark=True)

---------------------------------------------------------------------------
Py4JJavaError                             Traceback (most recent call last)
<ipython-input-30-a51b6ea6e5d2> in <module>
      1 db.import_existing_table('gaia', 'file:///data/gaia/dr2', num_buckets=500, zone_height=Constants.ONE_AMIN,
----> 2     import_into_spark=True)

/opt/spark/python/axs/catalog.py in import_existing_table(self, table_name, path, num_buckets, zone_height, import_into_spark, update_spark_bucketing, bucket_col, ra_col, dec_col, lightcurves, lc_cols)
     83             self.spark.catalog.createTable(table_name, path, "parquet")
     84         if update_spark_bucketing:
---> 85             self._CatalogUtils.updateSparkMetastoreBucketing(table_name, num_buckets)
     86         self._CatalogUtils.saveNewTable(table_name, num_buckets, zone_height,
     87                                         bucket_col, ra_col, dec_col, lightcurves, lc_cols)

/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py in __call__(self, *args)
   1255         answer = self.gateway_client.send_command(command)
   1256         return_value = get_return_value(
-> 1257             answer, self.gateway_client, self.target_id, self.name)
   1258 
   1259         for temp_arg in temp_args:

/opt/spark/python/lib/pyspark.zip/pyspark/sql/utils.py in deco(*a, **kw)
     61     def deco(*a, **kw):
     62         try:
---> 63             return f(*a, **kw)
     64         except py4j.protocol.Py4JJavaError as e:
     65             s = e.java_exception.toString()

/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py in get_return_value(answer, gateway_client, target_id, name)
    326                 raise Py4JJavaError(
    327                     "An error occurred while calling {0}{1}{2}.\n".
--> 328                     format(target_id, ".", name), value)
    329             else:
    330                 raise Py4JError(

Py4JJavaError: An error occurred while calling o72.updateSparkMetastoreBucketing.
: java.sql.SQLSyntaxErrorException: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source)
	at org.dirac.axs.util.CatalogUtils.getSparkTableId(CatalogUtils.java:134)
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR 42X05: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source)
	at org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source)
	at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source)
	... 15 more


# Something to note, when we investigated the metastore above, the table was named APP.AXSTABLES, but in the exeption, we see an error about APP.TBLS


# If we look at the AXS documentation this seems appropriate:

AXS data is physically stored in Parquet files (as was described in the introduction, and the information about them is stored in the AXS “metastore”.

AXS metastore relies on, and extends, Spark’s metastore. By default, Spark (and AXS) will store metastore data (information about tables, their physical location, bucketing information, and so on) in a Derby database, created in a folder called metastore_db in your working folder. You can also specify a different database to be used as a metastore by placing a hive-site.xml file, containing the required configuration, in the conf sub-directory of your AXS installation folder. AXS installation contains an example of this file for connecting to a MariaDB.

So, the goal is to have catalog data properly partitioned in Parquet files, on a local or a distributed file system, and information about the table data registered in the Spark metastore. Additionally, AXS also needs to register the table data in its own configuration tables.

# So perhaps we're missing a step here?

# Also useful information related to issue here:
# https://github.com/astronomy-commons/axs/issues/1


" Table TBLS is a Spark-generated (or rather Hive-generated) metastore table that contains info about tables that Spark manages. AXS reads this information and augments it with information in its own metastore table called AXSTABLES. So this is not an error and the problem is somewhere else. "



# Start spark-shell to test out a proposed debug

$SPARK_HOME/bin/spark-shell

>	20/12/17 15:08:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
	Setting default log level to "WARN".
	To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
	20/12/17 15:08:22 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
	20/12/17 15:08:23 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.

# Hangs here


# After some debugging, it looks like spark shell tries to start a Yarn application, but we already have an application using up all resources, so it never starts..
# (For debugging, we set the log status to "INFO" in Spark



# Check metastore tables through spark-shell based on comment in above github post
# fedora@master01
# -------------------------------------------------------------------------------
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/
         
Using Scala version 2.11.12 (OpenJDK 64-Bit Server VM, Java 1.8.0_252)
Type in expressions to have them evaluated.
Type :help for more information.

scala> Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
res0: Class[_] = class org.apache.derby.jdbc.EmbeddedDriver

scala> val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
conn: java.sql.Connection = org.apache.derby.impl.jdbc.EmbedConnection@221632586 (XID = 192), (SESSIONID = 1), (DATABASE = metastore_db), (DRDAID = null)

scala> val stmt = conn.createStatement()
stmt: java.sql.Statement = org.apache.derby.impl.jdbc.EmbedStatement@66b9f727

scala> val rs = stmt.executeQuery("select s.SCHEMANAME, t.TABLENAME from SYS.SYSTABLES t, SYS.SYSSCHEMAS s where t.schemaid = s.schemaid")
rs: java.sql.ResultSet = org.apache.derby.impl.jdbc.EmbedResultSet42@3b75fdd0

scala> while(rs.next()) {
     | println(rs.getString(1)+"."+rs.getString(2))
     | }
APP.AXSTABLES
SYS.SYSALIASES
SYS.SYSCHECKS
SYS.SYSCOLPERMS
SYS.SYSCOLUMNS
SYS.SYSCONGLOMERATES
SYS.SYSCONSTRAINTS
SYS.SYSDEPENDS
SYSIBM.SYSDUMMY1
SYS.SYSFILES
SYS.SYSFOREIGNKEYS
SYS.SYSKEYS
SYS.SYSPERMS
SYS.SYSROLES
SYS.SYSROUTINEPERMS
SYS.SYSSCHEMAS
SYS.SYSSEQUENCES
SYS.SYSSTATEMENTS
SYS.SYSSTATISTICS
SYS.SYSTABLEPERMS
SYS.SYSTABLES
SYS.SYSTRIGGERS
SYS.SYSUSERS
SYS.SYSVIEWS



# Starting to realise we have to learn about Hive and how to configure it..


# Here's me trying several different iterations of setting up a hive-site.xml file under /opt/spark/conf/

# --------------- 

<?xml version="1.0"?>
<configuration>
<property>
  <name>javax.jdo.option.ConnectionURL</name>
  <value>jdbc:derby:;databaseName=/opt/spark/metastore_db;create=true</value>
  <description>JDBC connect string for a JDBC metastore</description>
</property>

<property>
  <name>datanucleus.autoCreateSchema</name>
  <value>true</value>
</property>

<property>
  <name>datanucleus.fixedDatastore</name>
  <value>false</value>
</property>

</configuration>


# Produces following exception

.. # Long list of exceptions

Caused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/opt/spark/metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------


...

Caused by: ERROR XJ040: Failed to start database '/opt/spark/metastore_db' with class loader org.apache.spark.sql.hive.client.IsolatedClientLoader$$anon$1@2b0296f1, see the next exception for details.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)
	... 118 more
Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/metastore_db.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)


# Try deleting /opt/spark/metastore_db and try again

# Exception we've seen before..

Py4JJavaError: An error occurred while calling o140.updateSparkMetastoreBucketing.
: java.sql.SQLSyntaxErrorException: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source)
	at org.dirac.axs.util.CatalogUtils.getSparkTableId(CatalogUtils.java:134)
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR 42X05: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source)
	at org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source)
	at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source)
	... 15 more


# I'm quite frequently getting the error above:

Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/metastore_db.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)

# Let's try this:
   https://stackoverflow.com/questions/40396634/zeppelin-using-isolated-mode-for-spark-interpreter

# Uncheck zeppelin.spark.useHiveContext in Spark interpreter and see what happens



# Seems better?


# Update the name in the Hive metastore from AXSTABLES to TBLS and see if this fixes the errors:
# fedora@master01
# ---------------------------------------------------------------------------------------------------------------------
spark
Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeUpdate("RENAME TABLE APP.AXSTABLES TO TBLS")
val rs = stmt.executeQuery("select s.SCHEMANAME, t.TABLENAME from SYS.SYSTABLES t, SYS.SYSSCHEMAS s where t.schemaid = s.schemaid")
while(rs.next()) {
     | println(rs.getString(1)+"."+rs.getString(2))
     | }
SYS.SYSALIASES
SYS.SYSCHECKS
SYS.SYSCOLPERMS
SYS.SYSCOLUMNS
SYS.SYSCONGLOMERATES
SYS.SYSCONSTRAINTS
SYS.SYSDEPENDS
SYSIBM.SYSDUMMY1
SYS.SYSFILES
SYS.SYSFOREIGNKEYS
SYS.SYSKEYS
SYS.SYSPERMS
SYS.SYSROLES
SYS.SYSROUTINEPERMS
SYS.SYSSCHEMAS
SYS.SYSSEQUENCES
SYS.SYSSTATEMENTS
SYS.SYSSTATISTICS
SYS.SYSTABLEPERMS
SYS.SYSTABLES
SYS.SYSTRIGGERS
SYS.SYSUSERS
SYS.SYSVIEWS
APP.TBLS



# Now run the Zeppelin cell again
..

%spark.pyspark
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
#db.import_existing_table('gaia4', 'file:///data/gaia/dr2/part-02130-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet', num_buckets=500, zone_height=Constants.ONE_AMIN,
#    import_into_spark=True)

Py4JJavaError: An error occurred while calling o72.updateSparkMetastoreBucketing.
: java.lang.Exception: Table gaia does not exist.
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)



# New error, but progress maybe?
..

# After reading through the documentation again:

"""
** Importing new data from scratch **

Finally, you can also import new data into AXS and specify it as a Spark DataFrame. You use AxsCatalog’s save_axs_table method for that, which looks like this:

def save_axs_table(self, df, tblname, repartition=True, calculate_zone=False,
                   num_buckets=Constants.NUM_BUCKETS, zone_height=ZONE_HEIGHT):
As a bare minimum, you need to provide a Spark DataFrame (variable df), containing at least ra and dec columns, and the desired table name. You would typically obtain the DataFrame by loading and parsing data from external files (e.g. FITS or HDF5 files). If the data is already correctly partitioned (in Spark partitions), set repartition to False. If the zone column is already present in the input DataFrame, set calculate_zone to False. Finally, it is best to leave the defaults for the number of buckets and zone height, but you can also override the defaults.

"""

# Is this what we want?
# Let's try save_axs_table on a dataframe:


%spark.pyspark
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
df = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("parallax_error is not null")
df.createOrReplaceTempView("gaia")
db.save_axs_table(df, "gaiadr2", repartition=True, calculate_zone=True)


Py4JJavaError: An error occurred while calling o237.saveNewTable.
: java.lang.Exception: Spark table gaiadr2 not found!
	at org.dirac.axs.util.CatalogUtils.saveNewTable(CatalogUtils.java:205)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)



# If we run the same cell again:

Py4JJavaError: An error occurred while calling o168.saveAsTable.
: org.apache.spark.sql.AnalysisException: Table `gaiadr2` already exists.;
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:424)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


# After trying a couple of restarts, also got this error:



Py4JJavaError: An error occurred while calling o220.saveAsTable.
: org.apache.spark.sql.AnalysisException: Can not create the managed table('`gaiadr2`'). The associated location('/opt/spark/warehouse/gaiadr2') already exists.;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)



# Ok going back to the AXS documentation again because..confused

"""
** Importing data from Parquet files ** 

You can download catalog data as pre-partitioned AXS Parquet files. After placing the files in a folder (for example /data/axs/sdss), load the files into AXS metastore by calling AxsCatalog’s import_existing_table method:

from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
db.import_existing_table('sdss_table', '/data/axs/sdss', num_buckets=500, zone_height=Constants.ONE_AMIN,
    import_into_spark=True)

This will register the files from the /data/axs/sdss folder as a new table (visible both from Spark and AXS) called sdss_table and tell Spark that the files are bucketed by the zone column into 500 buckets and that the data within the buckets is sorted by zone and ra columns. When using this method it is assumed that there is no table with the same name in the metastore and that the Parquet files are indeed bucketed as described.
"""


# Try it once more and log the exception:

%spark.pyspark

db.import_existing_table('gaia_source', 'file:///data/gaia/dr2/part-02130-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet', num_buckets=500, zone_height=Constants.ONE_AMIN,
    import_into_spark=True)


Py4JJavaError: An error occurred while calling o72.updateSparkMetastoreBucketing.
: java.lang.Exception: Table gaia_source does not exist.
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)






