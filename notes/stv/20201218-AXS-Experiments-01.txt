#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#

Target:
 
  Create custom distribution of Axs-spark and deploy on our cluster


# This is a continuation of experiments described here:	https://github.com/stvoutsin/aglais/blob/issue-157/notes/stv/20201216-AXS-Experiments-01.txt

# What we have currently:
 # Spark/Yarn cluster with the Spark version being an AXS distribution which we compiled from source


# What we are trying currently:
  # Trying to get AXS working in Zeppelin/Spark to load tables from Parquet



# Load and init AXS
# user@zeppelin
# ------------------------------------------------ 

%spark.pyspark
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)

# SUCCESS


# Load parquet into DF
# user@zeppelin
# ------------------------------------------------ 

%spark.pyspark
df = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("parallax_error is not null").limit(1000)
df.createOrReplaceTempView("gaia")

# SUCCESS


# Save from Parquet to AXS?
# user@zeppelin
# ------------------------------------------------ 

%spark.pyspark
db.save_axs_table(df, "gaia_source", repartition=False, calculate_zone=True)


Py4JJavaError: An error occurred while calling o746.saveAsTable.
: org.apache.spark.sql.AnalysisException: Table `gaia_source` already exists.;
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:424)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


# ERROR 
# org.apache.spark.sql.AnalysisException: Table `gaia_source` already exists.;


# Try a different name

%spark.pyspark
db.save_axs_table(df, "gaia_source2", repartition=False, calculate_zone=True)

# Takes a while to complete, so it is doing something

Py4JJavaError: An error occurred while calling o72.saveNewTable.
: java.lang.Exception: Spark table gaia_source2 not found!
	at org.dirac.axs.util.CatalogUtils.saveNewTable(CatalogUtils.java:205)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


#  Spark table gaia_source2 not found ?



Py4JJavaError: An error occurred while calling o72.saveNewTable.
: java.lang.Exception: Spark table gaianew2 not found!
	at org.dirac.axs.util.CatalogUtils.saveNewTable(CatalogUtils.java:205)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)



# Could it be that Zeppelin / Spark (AXS) are looking at different Hive/Derby Databases?
# fedora@zeppelin
# ----------------------------------------------------------------

ls /home/fedora/zeppelin-0.8.2-bin-all/
LICENSE  NOTICE  README.md  bin  conf  derby.log  interpreter  lib  licenses  local-repo  logs  metastore_db  notebook  run  webapps  zeppelin-web-0.8.2.war

ls /opt/spark
LICENSE  NOTICE  README.md  RELEASE  bin  conf  data  derby.log  examples  jars  kubernetes  licenses  local  metastore_db  python  sbin  warehouse  yarn


# Two instances of metastore_db, which one is Zeppelin and/or Spark using?



# Check what is in the metastore_db Derby Database of Zeppelin:
# fedora@zeppelin
# ----------------------------------------------------------------
pushd /home/fedora/zeppelin-0.8.2-bin-all/


Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select s.SCHEMANAME, t.TABLENAME from SYS.SYSTABLES t, SYS.SYSSCHEMAS s where t.schemaid = s.schemaid")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}

	APP.AXSTABLES
	SYS.SYSALIASES
	SYS.SYSCHECKS
	SYS.SYSCOLPERMS
	SYS.SYSCOLUMNS
	SYS.SYSCONGLOMERATES
	SYS.SYSCONSTRAINTS
	SYS.SYSDEPENDS
	SYSIBM.SYSDUMMY1
	SYS.SYSFILES
	SYS.SYSFOREIGNKEYS
	SYS.SYSKEYS
	SYS.SYSPERMS
	SYS.SYSROLES
	SYS.SYSROUTINEPERMS
	SYS.SYSSCHEMAS
	SYS.SYSSEQUENCES
	SYS.SYSSTATEMENTS
	SYS.SYSSTATISTICS
	SYS.SYSTABLEPERMS
	SYS.SYSTABLES
	SYS.SYSTRIGGERS
	SYS.SYSUSERS
	SYS.SYSVIEWS


Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select * from APP.AXSTABLES")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}

# Nothing returned

popd

# Check what is in the metastore_db Derby Database of Spark:
# fedora@zeppelin
# ----------------------------------------------------------------
pushd /opt/spark/ 

Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select s.SCHEMANAME, t.TABLENAME from SYS.SYSTABLES t, SYS.SYSSCHEMAS s where t.schemaid = s.schemaid")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}

	APP.AXSTABLES
	SYS.SYSALIASES
	SYS.SYSCHECKS
	SYS.SYSCOLPERMS
	SYS.SYSCOLUMNS
	SYS.SYSCONGLOMERATES
	SYS.SYSCONSTRAINTS
	SYS.SYSDEPENDS
	SYSIBM.SYSDUMMY1
	SYS.SYSFILES
	SYS.SYSFOREIGNKEYS
	SYS.SYSKEYS
	SYS.SYSPERMS
	SYS.SYSROLES
	SYS.SYSROUTINEPERMS
	SYS.SYSSCHEMAS
	SYS.SYSSEQUENCES
	SYS.SYSSTATEMENTS
	SYS.SYSSTATISTICS
	SYS.SYSTABLEPERMS
	SYS.SYSTABLES
	SYS.SYSTRIGGERS
	SYS.SYSUSERS
	SYS.SYSVIEWS
	APP.TBLS

# One additional table, APP.TBLS	

Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select * from APP.AXSTABLES")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}

	# Nothing returned


Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select * from APP.TBLS")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}


	# Nothing returned	

:quit

popd




# If APP.TBLS does not exist, perhaps we can create it as a copy of AXSTABLES, as it seems that AXSTABLES is an extension of that?


Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeUpdate("CREATE TABLE APP.TBLS AS SELECT * FROM APP.AXSTABLES WITH no data")




# Run AXS loading from Pyspark shell (Not Zeppelin)
# fedora@zeppelin
# -------------------------------------------------------------
pyspark

from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
df = spark.read.parquet("file:///data/gaia/dr2/part-02130-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet").where("parallax_error is not null").limit(100)
db.save_axs_table(df, "gaia_source", repartition=False, calculate_zone=True)


..
20/12/18 16:46:02 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/12/18 16:46:02 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/12/18 16:46:02 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
Traceback (most recent call last):
  File "/opt/spark/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o143.saveAsTable.
: org.apache.spark.sql.AnalysisException: Can not create the managed table('`gaia_source`'). The associated location('hdfs://master01:9000/opt/spark/metastore_db/gaia_source') already exists.;
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.validateTableLocation(SessionCatalog.scala:336)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:170)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)




# Check HDFS 
# fedora@master01
# ---------------------------
hdfs dfs -ls /opt/spark
Found 3 items
drwxr-xr-x   - fedora supergroup          0 2020-12-16 19:02 /opt/spark/local
drwxr-xr-x   - fedora supergroup          0 2020-12-18 16:39 /opt/spark/metastore_db
drwxr-xr-x   - fedora supergroup          0 2020-12-18 16:16 /opt/spark/warehouse



# It looks like Spark is writing files to Hive db files to HDFS?
# This directory is picked up from our spark-default.conf

 spark.sql.warehouse.dir=/opt/spark/warehouse 
 #spark.sql.warehouse.dir=/opt/spark/metastore_db


hdfs dfs -ls /opt/spark/metastore_db/gaia_source
Found 47 items
-rw-r--r--   2 fedora supergroup          0 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/_SUCCESS
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00018.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00212.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23692 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00242.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23965 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00249.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00275.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00343.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00373.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00468.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00481.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00000-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00498.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25789 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00007.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24251 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00035.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24658 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00056.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24919 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00072.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00099.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00159.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24494 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00169.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24227 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00189.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23638 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00191.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26563 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00211.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25454 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00212.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00234.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00238.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00239.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00242.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00249.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24114 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00255.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23691 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00275.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00298.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24086 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00342.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24751 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00343.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26359 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00348.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23673 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00361.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24792 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00364.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24094 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00373.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00386.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00403.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00407.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00436.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00466.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00468.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25445 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00471.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26176 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00481.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24426 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00482.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00489.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23639 2020-12-18 16:39 /opt/spark/metastore_db/gaia_source/part-00001-a7a1f60f-9390-4bcf-aa08-a9813b2559e2_00498.c000.snappy.parquet



# Ok, so it seems that AXS did actually write out some parquet files to a Spark "warehouse", not sure which one of the examples did the trick, remove these HDFS directories and try again

# First set the spark warehouse dir to /warehouse
# Repeat in Zeppelin & Master node
# fedora@master01 & fedora@zeppelin
# ---------------------------------------------
nano /opt/spark/conf/spark-defaults.conf
..
  spark.sql.warehouse.dir=/warehouse
..

# Remove directories in HDFS
# fedora@master01
# ---------------------------------------------
hadoop fs -rm -f -r /opt/spark/metastore_db
hadoop fs -rm -f -r /opt/spark/warehouse


# Restart Hadoop
# fedora@master01
# ----------------------------------------------
stop-all.sh
start-all.sh


# Restart Zeppelin
# fedora@zeppelin
# ----------------------------------------------
/home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh restart



# Run examples in Pyspark again..#
# fedora@zeppelin
# ----------------------------------------------

pyspark

from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
df = spark.read.parquet("file:///data/gaia/dr2/part-02130-70392076-8b82-4457-8828-22069e7626e9-c000.snappy.parquet").where("parallax_error is not null").limit(100)
db.save_axs_table(df, "gaia_source", repartition=False, calculate_zone=True)


..

# Exception again..

 Caused by: ERROR XSDB6: Another instance of Derby may have already booted the database /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/metastore_db.

..

# This is after removing the metastore_db directory..

# What if we remove the locks by removing them and trying the last line again..

# In a separate console:
sudo rm /opt/spark/metastore_db/db*


# Back in Pyspark shell:
db.save_axs_table(df, "gaia_source", repartition=False, calculate_zone=True)

20/12/18 17:12:50 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
20/12/18 17:12:50 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
20/12/18 17:12:50 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
20/12/18 17:12:57 WARN HiveExternalCatalog: Persisting bucketed data source table `default`.`gaia_source` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive. 
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/axs/catalog.py", line 200, in save_axs_table
    False, None)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o44.saveNewTable.
: java.sql.SQLSyntaxErrorException: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source)
	at org.dirac.axs.util.CatalogUtils.getSparkTableId(CatalogUtils.java:134)
	at org.dirac.axs.util.CatalogUtils.saveNewTable(CatalogUtils.java:203)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR 42X05: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindTableDescriptor(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromBaseTable.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.FromList.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.SelectNode.bindNonVTITables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bindTables(Unknown Source)
	at org.apache.derby.impl.sql.compile.DMLStatementNode.bind(Unknown Source)
	at org.apache.derby.impl.sql.compile.CursorNode.bindStatement(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepMinion(Unknown Source)
	at org.apache.derby.impl.sql.GenericStatement.prepare(Unknown Source)
	at org.apache.derby.impl.sql.conn.GenericLanguageConnectionContext.prepareInternalStatement(Unknown Source)
	... 15 more



# Check HDFS again

hdfs dfs -ls /warehouse/gaia_source
Found 47 items
-rw-r--r--   2 fedora supergroup          0 2020-12-18 17:12 /warehouse/gaia_source/_SUCCESS
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00018.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00212.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23692 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00242.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23965 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00249.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00275.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00343.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00373.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00468.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00481.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00000-24b298ef-d020-4a46-b1b7-ece540894129_00498.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25789 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00007.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24251 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00035.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24658 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00056.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24919 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00072.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00099.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00159.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24494 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00169.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24227 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00189.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23638 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00191.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26563 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00211.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25454 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00212.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00234.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00238.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00239.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00242.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00249.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24114 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00255.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23691 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00275.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00298.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24086 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00342.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24751 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00343.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26359 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00348.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23673 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00361.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24792 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00364.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24094 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00373.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00386.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00403.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00407.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00436.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00466.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00468.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      25445 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00471.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      26176 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00481.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      24426 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00482.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23134 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00489.c000.snappy.parquet
-rw-r--r--   2 fedora supergroup      23639 2020-12-18 17:12 /warehouse/gaia_source/part-00001-24b298ef-d020-4a46-b1b7-ece540894129_00498.c000.snappy.parquet


# Something was indeed written to the warehouse, even though we got an exception

# From the github issue on the subject:
# https://github.com/astronomy-commons/axs/issues/1

"""
Table TBLS is a Spark-generated (or rather Hive-generated) metastore table that contains info about tables that Spark manages. AXS reads this information and augments it with information in its own metastore table called AXSTABLES. So this is not an error and the problem is somewhere else.
"""

# If we looks at the metastore_db database again, what is in there?

pushd /opt/spark/

spark-shell
Class.forName("org.apache.derby.jdbc.EmbeddedDriver")
val conn = java.sql.DriverManager.getConnection("jdbc:derby:metastore_db;create=true", "", "")
val stmt = conn.createStatement()
val rs = stmt.executeQuery("select s.SCHEMANAME, t.TABLENAME from SYS.SYSTABLES t, SYS.SYSSCHEMAS s where t.schemaid = s.schemaid")
while(rs.next()) {
println(rs.getString(1)+"."+rs.getString(2))
}

APP.AXSTABLES
APP.BUCKETING_COLS
APP.CDS
APP.COLUMNS_V2
APP.DATABASE_PARAMS
APP.DBS
APP.FUNCS
APP.FUNC_RU
APP.GLOBAL_PRIVS
APP.PARTITIONS
APP.PARTITION_KEYS
APP.PARTITION_KEY_VALS
APP.PARTITION_PARAMS
APP.PART_COL_STATS
APP.ROLES
APP.SDS
APP.SD_PARAMS
APP.SEQUENCE_TABLE
APP.SERDES
APP.SERDE_PARAMS
APP.SKEWED_COL_NAMES
APP.SKEWED_COL_VALUE_LOC_MAP
APP.SKEWED_STRING_LIST
APP.SKEWED_STRING_LIST_VALUES
APP.SKEWED_VALUES
APP.SORT_COLS
SYS.SYSALIASES
SYS.SYSCHECKS
SYS.SYSCOLPERMS
SYS.SYSCOLUMNS
SYS.SYSCONGLOMERATES
SYS.SYSCONSTRAINTS
SYS.SYSDEPENDS
SYSIBM.SYSDUMMY1
SYS.SYSFILES
SYS.SYSFOREIGNKEYS
SYS.SYSKEYS
SYS.SYSPERMS
SYS.SYSROLES
SYS.SYSROUTINEPERMS
SYS.SYSSCHEMAS
SYS.SYSSEQUENCES
SYS.SYSSTATEMENTS
SYS.SYSSTATISTICS
SYS.SYSTABLEPERMS
SYS.SYSTABLES
SYS.SYSTRIGGERS
SYS.SYSUSERS
SYS.SYSVIEWS
APP.TABLE_PARAMS
APP.TAB_COL_STATS
APP.TBLS
APP.VERSION


:quit

popd


# Both APP.TBLS & APP.AXSTABLES exists, so why the error above??


# Back in pyspark shell, see if we can load from the Spark Warehouse
# fedora@zeppelin
# -------------------------------------------------------------------

db.import_existing_table('gaia_source', '', num_buckets=500, zone_height=Constants.ONE_AMIN,
     import_into_spark=False, update_spark_bucketing=True)

> # No exceptions, or output


# List tables in db
for tname in db.list_tables():
     print(tname)
 
> gaia_source



# Load Tables
gaia = db.load('gaia_source')
> 20/12/18 17:35:03 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException

# If we wanted to crossmatch, we could do:
# sdss = db.load('sdss')

# gaia_sdss_cm = gaia.crossmatch(sdss, 2*Constants.ONE_ASEC,
#    return_min=False, include_dist_col=True)






# Ok, what happens if we clean the metastore database, and start pyspark. 
# Can we access the gaia_source Parquet files through AXS?
# fedora@zeppelin
# -------------------------------------------------------------------

pyspark
>>> from axs import AxsCatalog, Constants
>>> db = AxsCatalog(spark)
>>> db.import_existing_table('gaia_source', '', num_buckets=500, zone_height=Constants.ONE_AMIN,
...      import_into_spark=False, update_spark_bucketing=True)
Traceback (most recent call last):
  File "<stdin>", line 2, in <module>
  File "/opt/spark/python/axs/catalog.py", line 85, in import_existing_table
    self._CatalogUtils.updateSparkMetastoreBucketing(table_name, num_buckets)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o44.updateSparkMetastoreBucketing.
: java.sql.SQLSyntaxErrorException: Table/View 'APP.TBLS' does not exist.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeQuery(Unknown Source)
	at org.dirac.axs.util.CatalogUtils.getSparkTableId(CatalogUtils.java:134)
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR 42X05: Table/View 'APP.TBLS' does not exist.


# Try creating the APP.TBLS table as described above and try again..
# CREATE TABLE APP.TBLS AS SELECT * FROM APP.AXSTABLES WITH no data 
# ----------------------------------------------------------------------------

pyspark 
from axs import AxsCatalog, Constants
db = AxsCatalog(spark)
db.import_existing_table('gaia_source', '', num_buckets=500, zone_height=Constants.ONE_AMIN, import_into_spark=False, update_spark_bucketing=True)
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
  File "/opt/spark/python/axs/catalog.py", line 85, in import_existing_table
    self._CatalogUtils.updateSparkMetastoreBucketing(table_name, num_buckets)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1257, in __call__
  File "/opt/spark/python/pyspark/sql/utils.py", line 63, in deco
    return f(*a, **kw)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
py4j.protocol.Py4JJavaError: An error occurred while calling o44.updateSparkMetastoreBucketing.
: java.lang.Exception: Table gaia_source does not exist.
	at org.dirac.axs.util.CatalogUtils.updateSparkMetastoreBucketing(CatalogUtils.java:60)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)


# Perhaps even though it exists in the Spark Warehouse, it no longer is registered in the metastore_db so we cant see it?
# If we clear the warehouse version of gaia_source, and try to run the save command again:

db.save_axs_table(df, "gaia_source", repartition=False, calculate_zone=True)

...

# Long list of exceptions....


Caused by: ERROR 42X14: 'SD_ID' is not a column in table or VTI 'TBLS'.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.impl.sql.execute.CreateIndexConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.CreateConstraintConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantActionBody(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.MiscResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 104 more

20/12/18 18:48:37 ERROR Datastore: An exception was thrown while adding/validating class(es) : Constraint 'TABLE_PARAMS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
java.sql.SQLException: Constraint 'TABLE_PARAMS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)
	at org.datanucleus.store.rdbms.table.TableImpl.createForeignKeys(TableImpl.java:527)
	at org.datanucleus.store.rdbms.table.TableImpl.createConstraints(TableImpl.java:423)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3459)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:814)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy34.createTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1449)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy36.create_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2050)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:97)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:669)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:657)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy37.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:714)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:399)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR X0Y44: Constraint 'TABLE_PARAMS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.sql.dictionary.DDUtils.locateReferencedConstraint(Unknown Source)
	at org.apache.derby.impl.sql.execute.CreateConstraintConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantActionBody(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.MiscResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 102 more

20/12/18 18:48:37 ERROR Datastore: An exception was thrown while adding/validating class(es) : Constraint 'PARTITION_KEYS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
java.sql.SQLException: Constraint 'PARTITION_KEYS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
	at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.Util.generateCsSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.wrapInSQLException(Unknown Source)
	at org.apache.derby.impl.jdbc.TransactionResourceImpl.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedConnection.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.ConnectionChild.handleException(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.executeStatement(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at org.apache.derby.impl.jdbc.EmbedStatement.execute(Unknown Source)
	at com.jolbox.bonecp.StatementHandle.execute(StatementHandle.java:254)
	at org.datanucleus.store.rdbms.table.AbstractTable.executeDdlStatement(AbstractTable.java:760)
	at org.datanucleus.store.rdbms.table.TableImpl.createForeignKeys(TableImpl.java:527)
	at org.datanucleus.store.rdbms.table.TableImpl.createConstraints(TableImpl.java:423)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.performTablesValidation(RDBMSStoreManager.java:3459)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.addClassTablesAndValidate(RDBMSStoreManager.java:3190)
	at org.datanucleus.store.rdbms.RDBMSStoreManager$ClassAdder.run(RDBMSStoreManager.java:2841)
	at org.datanucleus.store.rdbms.AbstractSchemaTransaction.execute(AbstractSchemaTransaction.java:122)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.addClasses(RDBMSStoreManager.java:1605)
	at org.datanucleus.store.AbstractStoreManager.addClass(AbstractStoreManager.java:954)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getDatastoreClass(RDBMSStoreManager.java:679)
	at org.datanucleus.store.rdbms.RDBMSStoreManager.getPropertiesForGenerator(RDBMSStoreManager.java:2045)
	at org.datanucleus.store.AbstractStoreManager.getStrategyValue(AbstractStoreManager.java:1365)
	at org.datanucleus.ExecutionContextImpl.newObjectId(ExecutionContextImpl.java:3827)
	at org.datanucleus.state.JDOStateManager.setIdentity(JDOStateManager.java:2571)
	at org.datanucleus.state.JDOStateManager.initialiseForPersistentNew(JDOStateManager.java:513)
	at org.datanucleus.state.ObjectProviderFactoryImpl.newForPersistentNew(ObjectProviderFactoryImpl.java:232)
	at org.datanucleus.ExecutionContextImpl.newObjectProviderForPersistentNew(ExecutionContextImpl.java:1414)
	at org.datanucleus.ExecutionContextImpl.persistObjectInternal(ExecutionContextImpl.java:2218)
	at org.datanucleus.ExecutionContextImpl.persistObjectWork(ExecutionContextImpl.java:2065)
	at org.datanucleus.ExecutionContextImpl.persistObject(ExecutionContextImpl.java:1913)
	at org.datanucleus.ExecutionContextThreadedImpl.persistObject(ExecutionContextThreadedImpl.java:217)
	at org.datanucleus.api.jdo.JDOPersistenceManager.jdoMakePersistent(JDOPersistenceManager.java:727)
	at org.datanucleus.api.jdo.JDOPersistenceManager.makePersistent(JDOPersistenceManager.java:752)
	at org.apache.hadoop.hive.metastore.ObjectStore.createTable(ObjectStore.java:814)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RawStoreProxy.invoke(RawStoreProxy.java:114)
	at com.sun.proxy.$Proxy34.createTable(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_core(HiveMetaStore.java:1416)
	at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.create_table_with_environment_context(HiveMetaStore.java:1449)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingHMSHandler.invoke(RetryingHMSHandler.java:107)
	at com.sun.proxy.$Proxy36.create_table_with_environment_context(Unknown Source)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.create_table_with_environment_context(HiveMetaStoreClient.java:2050)
	at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.create_table_with_environment_context(SessionHiveMetaStoreClient.java:97)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:669)
	at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.createTable(HiveMetaStoreClient.java:657)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.invoke(RetryingMetaStoreClient.java:156)
	at com.sun.proxy.$Proxy37.createTable(Unknown Source)
	at org.apache.hadoop.hive.ql.metadata.Hive.createTable(Hive.java:714)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply$mcV$sp(HiveClientImpl.scala:482)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$createTable$1.apply(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.client.HiveClientImpl$$anonfun$withHiveState$1.apply(HiveClientImpl.scala:275)
	at org.apache.spark.sql.hive.client.HiveClientImpl.liftedTree1$1(HiveClientImpl.scala:213)
	at org.apache.spark.sql.hive.client.HiveClientImpl.retryLocked(HiveClientImpl.scala:212)
	at org.apache.spark.sql.hive.client.HiveClientImpl.withHiveState(HiveClientImpl.scala:258)
	at org.apache.spark.sql.hive.client.HiveClientImpl.createTable(HiveClientImpl.scala:480)
	at org.apache.spark.sql.hive.HiveExternalCatalog.saveTableIntoHive(HiveExternalCatalog.scala:499)
	at org.apache.spark.sql.hive.HiveExternalCatalog.org$apache$spark$sql$hive$HiveExternalCatalog$$createDataSourceTable(HiveExternalCatalog.scala:399)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply$mcV$sp(HiveExternalCatalog.scala:263)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$createTable$1.apply(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:97)
	at org.apache.spark.sql.hive.HiveExternalCatalog.createTable(HiveExternalCatalog.scala:236)
	at org.apache.spark.sql.catalyst.catalog.ExternalCatalogWithListener.createTable(ExternalCatalogWithListener.scala:94)
	at org.apache.spark.sql.catalyst.catalog.SessionCatalog.createTable(SessionCatalog.scala:324)
	at org.apache.spark.sql.execution.command.CreateDataSourceTableAsSelectCommand.run(createDataSourceTables.scala:185)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)
	at org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)
	at org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)
	at org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)
	at org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:676)
	at org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:78)
	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:125)
	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:73)
	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:676)
	at org.apache.spark.sql.DataFrameWriter.createTable(DataFrameWriter.scala:474)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:453)
	at org.apache.spark.sql.DataFrameWriter.saveAsTable(DataFrameWriter.scala:409)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
Caused by: ERROR X0Y44: Constraint 'PARTITION_KEYS_FK1' is invalid: there is no unique or primary key constraint on table '"APP"."TBLS"' that matches the number and types of the columns in the foreign key.
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)
	at org.apache.derby.iapi.sql.dictionary.DDUtils.locateReferencedConstraint(Unknown Source)
	at org.apache.derby.impl.sql.execute.CreateConstraintConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantActionBody(Unknown Source)
	at org.apache.derby.impl.sql.execute.AlterTableConstantAction.executeConstantAction(Unknown Source)
	at org.apache.derby.impl.sql.execute.MiscResultSet.open(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.executeStmt(Unknown Source)
	at org.apache.derby.impl.sql.GenericPreparedStatement.execute(Unknown Source)
	... 102 more


# Looks like creating TBLS as a copy didnt quite work???




