#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#


# On an existing Zeppelin/Spark cluster with AXS (see: 20201231-AXS-with-Hive.txt), try replacing the Spark installation with the latest from AXS

# The latest release which is references here: https://axs.readthedocs.io/en/latest/ can be found here:
https://github.com/astronomy-commons/axs/releases/download/v1.0/axs-distribution.tar.gz


# -------------------------------------------------------------------
# Check that pyspark works before changing everything:
# fedora@zeppelin


pyspark
Python 3.7.7 (default, Mar 13 2020, 21:39:43) 
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/01/18 16:11:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
Setting default log level to "WARN".
To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
21/01/18 16:11:23 WARN spark.SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/18 16:11:24 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.3
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.



# -------------------------------------------------------------------
# Try replacing the Spark installation in our Zeppelin node:
# fedora@zeppelin

sudo su
pushd /opt	
    mv spark-3.0.0-preview-bin-AXS-v3.0.0-preview spark-2.4.7
    wget https://github.com/astronomy-commons/axs/releases/download/v1.0/axs-distribution.tar.gz
    tar -xzvf axs-distribution.tar.gz
    rm -f axs-distribution.tar.gz 
    mv axs-dist/ spark-3.0.0-preview-bin-AXS-v3.0.0-preview
    chown -R fedora:root spark-3.0.0-preview-bin-AXS-v3.0.0-preview/

popd



# -------------------------------------------------------------------
# Check what is in spark-defaults of latest AXS distr.
# fedora@zeppelin

#
# Licensed to the Apache Software Foundation (ASF) under one or more
# contributor license agreements.  See the NOTICE file distributed with
# this work for additional information regarding copyright ownership.
# The ASF licenses this file to You under the Apache License, Version 2.0
# (the "License"); you may not use this file except in compliance with
# the License.  You may obtain a copy of the License at
#
#    http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
#

# Default system properties included when running spark-submit.
# This is useful for setting default environmental settings.

# Example:
# spark.master                     spark://master:7077
# spark.eventLog.enabled           true
# spark.eventLog.dir               hdfs://namenode:8021/directory
# spark.serializer                 org.apache.spark.serializer.KryoSerializer
# spark.driver.memory              5g
# spark.executor.extraJavaOptions  -XX:+PrintGCDetails -Dkey=value -Dnumbers="one two three"
spark.local.dir        SPARK_HOME/work
spark.sql.warehouse.dir        file://SPARK_HOME/warehouse
spark.files.maxPartitionBytes	471859200
spark.memory.offHeap.enabled	true
spark.memory.offHeap.size	12g 
# spark.jars.packages	org.mariadb.jdbc:mariadb-java-client:2.2.3
spark.jars	SPARK_HOME/python/axs/AxsUtilities-1.0-SNAPSHOT.jar
spark.scheduler.minRegisteredResourcesRatio	0.75




# -------------------------------------------------------------------
# Copy Spark & Hive configuration from previous Spark install
# fedora@zeppelin


# /opt/spark/conf/spark-defaults.conf

spark.master            yarn
spark.driver.memory              7g
spark.yarn.am.memory            7g
spark.executor.memory          7g
spark.eventLog.enabled  true
spark.driver.maxResultSize8192m
spark.local.dir         /opt/spark/local
spark.executor.cores            4
spark.executor.instances    4
spark.yarn.am.cores  4
spark.eventLog.enabled  true
spark.eventLog.dirhdfs://master01:9000/spark-log
# END Ansible managed Spark configuration
# BEGIN Ansible managed Spark environment
# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
# END Ansible managed Spark environment
spark.sql.warehouse.dir=/warehouse
spark.files.maxPartitionBytes	471859200
spark.memory.offHeap.enabled	true
spark.memory.offHeap.size	5g 
spark.jars	/opt/spark/python/axs/AxsUtilities-1.0-SNAPSHOT.jar
spark.scheduler.minRegisteredResourcesRatio	0.75


# Copy hive-site.xml configuration file
sudo cp /opt/spark-2.4.7/conf/hive-site.xml /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/conf/



# -------------------------------------------------------------------
# Try running PySpark
# fedora@zeppelin

sorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
21/01/18 16:03:50 WARN SparkContext: Another SparkContext is being constructed (or threw an exception in its constructor).  This may indicate an error, since only one SparkContext may be running in this JVM (see SPARK-2243). The other SparkContext was created at:
org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
java.lang.reflect.Constructor.newInstance(Constructor.java:423)
py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
py4j.Gateway.invoke(Gateway.java:238)
py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
py4j.GatewayConnection.run(GatewayConnection.java:238)
java.lang.Thread.run(Thread.java:748)
21/01/18 16:03:50 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/18 16:03:50 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
21/01/18 16:03:50 ERROR SparkContext: Failed to add file:/opt/SPARK_HOME/python/axs/AxsUtilities-1.0-SNAPSHOT.jar to Spark environment
java.io.FileNotFoundException: Jar /opt/SPARK_HOME/python/axs/AxsUtilities-1.0-SNAPSHOT.jar not found
	at org.apache.spark.SparkContext.addJarFile$1(SparkContext.scala:1838)
	at org.apache.spark.SparkContext.addJar(SparkContext.scala:1868)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
	at org.apache.spark.SparkContext$$anonfun$12.apply(SparkContext.scala:458)
	at scala.collection.immutable.List.foreach(List.scala:392)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:458)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
/opt/spark/python/pyspark/shell.py:45: UserWarning: Failed to initialize Spark session.
  warnings.warn("Failed to initialize Spark session.")
Traceback (most recent call last):
  File "/opt/spark/python/pyspark/shell.py", line 41, in <module>
    spark = SparkSession._create_shell_session()
  File "/opt/spark/python/pyspark/sql/session.py", line 577, in _create_shell_session
    return SparkSession.builder.getOrCreate()
  File "/opt/spark/python/pyspark/sql/session.py", line 173, in getOrCreate
    sc = SparkContext.getOrCreate(sparkConf)
  File "/opt/spark/python/pyspark/context.py", line 344, in getOrCreate
    SparkContext(conf=conf or SparkConf())
  File "/opt/spark/python/pyspark/context.py", line 118, in __init__
    conf, jsc, profiler_cls)
  File "/opt/spark/python/pyspark/context.py", line 180, in _do_init
    self._jsc = jsc or self._initialize_context(self._conf._jconf)
  File "/opt/spark/python/pyspark/context.py", line 283, in _initialize_context
    return self._jvm.JavaSparkContext(jconf)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py", line 1525, in __call__
    answer, self._gateway_client, None, self._fqn)
  File "/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py", line 328, in get_return_value
    format(target_id, ".", name), value)
py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.api.java.JavaSparkContext.
: java.lang.NoSuchMethodError: org.apache.hadoop.io.retry.RetryPolicies.retryOtherThanRemoteException(Lorg/apache/hadoop/io/retry/RetryPolicy;Ljava/util/Map;)Lorg/apache/hadoop/io/retry/RetryPolicy;
	at org.apache.hadoop.yarn.client.RMProxy.createRetryPolicy(RMProxy.java:255)
	at org.apache.hadoop.yarn.client.RMProxy.createRMProxy(RMProxy.java:91)
	at org.apache.hadoop.yarn.client.ClientRMProxy.createRMProxy(ClientRMProxy.java:72)
	at org.apache.hadoop.yarn.client.api.impl.YarnClientImpl.serviceStart(YarnClientImpl.java:187)
	at org.apache.hadoop.service.AbstractService.start(AbstractService.java:193)
	at org.apache.spark.deploy.yarn.Client.submitApplication(Client.scala:161)
	at org.apache.spark.scheduler.cluster.YarnClientSchedulerBackend.start(YarnClientSchedulerBackend.scala:57)
	at org.apache.spark.scheduler.TaskSchedulerImpl.start(TaskSchedulerImpl.scala:178)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:501)
	at org.apache.spark.api.java.JavaSparkContext.<init>(JavaSparkContext.scala:58)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:423)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:247)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:238)
	at py4j.commands.ConstructorCommand.invokeConstructor(ConstructorCommand.java:80)
	at py4j.commands.ConstructorCommand.execute(ConstructorCommand.java:69)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)




# Check what jars are in the jars/ directory of the latest Spark distr.

pushd /opt/spark/jars

ls -al 

...
-rw-rw-r--.  1 fedora root    40471 Dec  3  2018 hadoop-annotations-2.6.5.jar
-rw-rw-r--.  1 fedora root    40863 Dec  3  2018 hadoop-annotations-2.7.3.jar
-rw-rw-r--.  1 fedora root    90610 Dec  3  2018 hadoop-auth-2.6.5.jar
-rw-rw-r--.  1 fedora root    94150 Dec  3  2018 hadoop-auth-2.7.3.jar
-rw-rw-r--.  1 fedora root    25986 Dec  3  2018 hadoop-client-2.6.5.jar
-rw-rw-r--.  1 fedora root    26012 Dec  3  2018 hadoop-client-2.7.3.jar
-rw-rw-r--.  1 fedora root  3354982 Dec  3  2018 hadoop-common-2.6.5.jar
-rw-rw-r--.  1 fedora root  3479293 Dec  3  2018 hadoop-common-2.7.3.jar
-rw-rw-r--.  1 fedora root  7944061 Dec  3  2018 hadoop-hdfs-2.6.5.jar
-rw-rw-r--.  1 fedora root  8316190 Dec  3  2018 hadoop-hdfs-2.7.3.jar
-rw-rw-r--.  1 fedora root   528586 Dec  3  2018 hadoop-mapreduce-client-app-2.6.5.jar
-rw-rw-r--.  1 fedora root   542869 Dec  3  2018 hadoop-mapreduce-client-app-2.7.3.jar
-rw-rw-r--.  1 fedora root   688209 Dec  3  2018 hadoop-mapreduce-client-common-2.6.5.jar
-rw-rw-r--.  1 fedora root   776634 Dec  3  2018 hadoop-mapreduce-client-common-2.7.3.jar
-rw-rw-r--.  1 fedora root  1537966 Dec  3  2018 hadoop-mapreduce-client-core-2.6.5.jar
-rw-rw-r--.  1 fedora root  1556539 Dec  3  2018 hadoop-mapreduce-client-core-2.7.3.jar
-rw-rw-r--.  1 fedora root    61307 Dec  3  2018 hadoop-mapreduce-client-jobclient-2.6.5.jar
-rw-rw-r--.  1 fedora root    62304 Dec  3  2018 hadoop-mapreduce-client-jobclient-2.7.3.jar
-rw-rw-r--.  1 fedora root    67772 Dec  3  2018 hadoop-mapreduce-client-shuffle-2.6.5.jar
-rw-rw-r--.  1 fedora root    71737 Dec  3  2018 hadoop-mapreduce-client-shuffle-2.7.3.jar
-rw-rw-r--.  1 fedora root  1896185 Dec  3  2018 hadoop-yarn-api-2.6.5.jar
-rw-rw-r--.  1 fedora root  2039143 Dec  3  2018 hadoop-yarn-api-2.7.3.jar
-rw-rw-r--.  1 fedora root   151865 Dec  3  2018 hadoop-yarn-client-2.6.5.jar
-rw-rw-r--.  1 fedora root   165867 Dec  3  2018 hadoop-yarn-client-2.7.3.jar
-rw-rw-r--.  1 fedora root  1629101 Dec  3  2018 hadoop-yarn-common-2.6.5.jar
-rw-rw-r--.  1 fedora root  1678642 Dec  3  2018 hadoop-yarn-common-2.7.3.jar
-rw-rw-r--.  1 fedora root   319959 Dec  3  2018 hadoop-yarn-server-common-2.6.5.jar
-rw-rw-r--.  1 fedora root   388235 Dec  3  2018 hadoop-yarn-server-common-2.7.3.jar
-rw-rw-r--.  1 fedora root    58407 Dec  3  2018 hadoop-yarn-server-web-proxy-2.7.3.jar

...
-rw-rw-r--.  1 fedora root 15612191 Dec  3  2018 scala-compiler-2.11.12.jar
-rw-rw-r--.  1 fedora root  5749423 Dec  3  2018 scala-library-2.11.12.jar
-rw-rw-r--.  1 fedora root   471925 Dec  3  2018 scala-parser-combinators_2.11-1.1.0.jar
-rw-rw-r--.  1 fedora root  4623075 Dec  3  2018 scala-reflect-2.11.12.jar
-rw-rw-r--.  1 fedora root   671138 Dec  3  2018 scala-xml_2.11-1.0.5.jar



# Looks like multiple instances of hadoop libs
# 2.7.3 & 2.6.5


# Try removing all 2.6.5 jars. and try running pyspark again

...


pyspark
Python 3.7.7 (default, Mar 13 2020, 21:39:43) 
[GCC 9.2.1 20190827 (Red Hat 9.2.1-1)] on linux
Type "help", "copyright", "credits" or "license" for more information.
21/01/18 18:35:41 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
21/01/18 18:35:42 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).
21/01/18 18:35:43 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0-SNAPSHOT
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.
>>> from axs import AxsCatalog, Constants

>>> db = AxsCatalog(spark)
>>> 
>>> 
>>> dfgaia = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("dec>89")
>>> df2mass = spark.read.parquet("file:////user/nch/PARQUET/TESTS/2MASS/*.parquet").where("dec>88")
>>>                                                                             
>>> 
>>> # drop the catalogues from previous runs if necessary:
>>> db.drop_table('gaia_source_sample')
java.lang.ClassNotFoundException: com.mysql.jdbc.Driver
	at java.net.URLClassLoader.findClass(URLClassLoader.java:382)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:418)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:351)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:264)
	at org.dirac.axs.util.CatalogUtils.getConnection(CatalogUtils.java:44)
	at org.dirac.axs.util.CatalogUtils.deleteTable(CatalogUtils.java:238)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)
	at py4j.Gateway.invoke(Gateway.java:282)
	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
	at py4j.commands.CallCommand.execute(CallCommand.java:79)
	at py4j.GatewayConnection.run(GatewayConnection.java:238)
	at java.lang.Thread.run(Thread.java:748)
An error occurred while calling o51.deleteTable.



# MySQL Driver missing, copy it over from previous deploy:

cp /opt/spark-2.4.7/jars/mysql-connector-java-8.0.22.jar /opt/spark-3.0.0-preview-bin-AXS-v3.0.0-preview/jars/



pyspark

Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /__ / .__/\_,_/_/ /_/\_\   version 2.4.0-SNAPSHOT
      /_/

Using Python version 3.7.7 (default, Mar 13 2020 21:39:43)
SparkSession available as 'spark'.

>> from axs import AxsCatalog, Constants
>>> db = AxsCatalog(spark)
>>> dfgaia = spark.read.parquet("file:///data/gaia/dr2/*.parquet").where("dec>89")
>>> df2mass = spark.read.parquet("file:////user/nch/PARQUET/TESTS/2MASS/*.parquet").where("dec>88")
>>> db.drop_table('gaia_source_sample')                                         
Loading class `com.mysql.jdbc.Driver'. This is deprecated. The new driver class is `com.mysql.cj.jdbc.Driver'. The driver is automatically registered via the SPI and manual loading of the driver class is generally unnecessary.
21/01/19 16:20:41 WARN ObjectStore: Failed to get database global_temp, returning NoSuchObjectException
>>> 
>>> 
>>> # drop the catalogues from previous runs if necessary:
>>> db.drop_table('gaia_source_sample')
Spark table gaia_source_sample ID not found. Deleting by name.
'Table or view not found: gaia_source_sample;'
>>> db.drop_table('twomass_sample')

>>> 
>>> 
>>> # Create new Parquet files in Spark Metastore
>>> db.save_axs_table(dfgaia, "gaia_source_sample", repartition=True, calculate_zone=True, path = 'file:///user/nch/PARQUET/AXS/GEDR3')
Traceback (most recent call last):
  File "<stdin>", line 1, in <module>
TypeError: save_axs_table() got an unexpected keyword argument 'path'


# So it seems that with this version the API's have changed? Is this a previous version than the one we have been testing?

# It works without the path:

>>> db.save_axs_table(df2mass, "twomass_sample", repartition=True, calculate_zone=True)
21/01/19 16:22:47 WARN Utils: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.debug.maxToStringFields' in SparkEnv.conf.
21/01/19 16:23:01 WARN HiveExternalCatalog: Persisting bucketed data source table `default`.`twomass_sample` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive. 
>>> db.save_axs_table(dfgaia, "gaia_source_sample", repartition=True, calculate_zone=True)
21/01/19 16:23:33 WARN HiveExternalCatalog: Persisting bucketed data source table `default`.`gaia_source_sample` into Hive metastore in Spark SQL specific format, which is NOT compatible with Hive. 

>>> gaia = db.load('gaia_source_sample')
>>> twomass = db.load('twomass_sample')
>>> 
>>> gaia.exclude_duplicates().count()
21254
>>> gaia = db.load('gaia_source_sample')
>>> twomass = db.load('twomass_sample')
>>> gaia_sdss_cm = gaia.crossmatch(twomass, 5.0*Constants.ONE_ASEC, return_min=False, include_dist_col=True)
>>> 




