#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
#
# </meta:header>
#
#


Target:

	Figure out how to configure temporary space
        Run Bulk data load notebook successfully

Result:

	Success



# Setup an Ansible deploy on "test"

#[user@desktop]

    cloudname=gaia-test

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
# (*) extra volume mount for /common
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator2 \
        --hostname kubernator \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash


# -----------------------------------------------------
# Delete everything.
#[root@ansibler]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Default locations for config and status.
#[root@ansibler]

    configyml=/tmp/aglais-config.yml
    statusyml=/tmp/aglais-status.yml


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@ansibler]

cat > "${configyml:?}" << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloudname: ${cloudname:?}
EOF


# -----------------------------------------------------
# Create everything.
#[root@ansibler]

    /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....

    #
    # Looks good.
    #



# Create notebook as replica of
http://zeppelin.aglais.uk:8080/#/notebook/2FV6XCUYT


# Check where Spark is writing temp files
# ------------------------------------------------
# fedora@zeppelin


cat /opt/spark/conf/spark-defaults.conf

	>
	# BEGIN Ansible managed Spark configuration
	# https://spark.apache.org/docs/3.0.0-preview2/running-on-yarn.html#spark-properties
	spark.master            yarn
	spark.driver.memory              17g
	spark.yarn.am.memory            17g
	spark.executor.memory          17g
	spark.executor.cores            4
	spark.executor.instances    4
	spark.yarn.am.cores  4
	spark.eventLog.enabled  true
	spark.driver.maxResultSize	8192m
	spark.local.dir         /opt/spark/local
	spark.master            yarn
	spark.eventLog.enabled  true
	spark.eventLog.dir      hdfs://master01:9000/spark-log
	# END Ansible managed Spark configuration
	# BEGIN Ansible managed Spark environment
	# https://spark.apache.org/docs/3.0.0-preview2/configuration.html#inheriting-hadoop-cluster-configuration
	spark.yarn.appMasterEnv.YARN_CONF_DIR=/opt/hadoop/etc/hadoop
	spark.yarn.appMasterEnv.HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop
	# END Ansible managed Spark environment


# ------------------------------------------------
# Run Pyspark notebook and check /opt/spark/local
# fedora@zeppelin

ls -al /opt/spark/local/
total 16
drwxrwxr-x.  4 fedora fedora 4096 Jan 27 17:41 .
drwxr-xr-x. 14 fedora fedora 4096 Jan 22 18:47 ..
drwxrwxr-x. 45 fedora fedora 4096 Jan 27 17:53 blockmgr-73d55595-89a9-43a5-bb1d-41722e2e0430
drwx------.  4 fedora fedora 4096 Jan 27 17:41 spark-5cab9934-9b31-4d70-a614-6344e30d6ce7


# Spark produces Exception at cell:

%pyspark

# new formatting of GEDR3 with HPX12 buckets:
gaia_source_df = sqlContext.read.option('mode','failfast').option('header', 'true').schema(gaia_source_schema).csv('file:////user/nch/CSV/GEDR3/*.csv')
saveToBinnedParquet(gaia_source_df, 'file:///user/nch/PARQUET/HPX12_BUCKETS/GEDR3', name = 'gaia_source', mode = 'overwrite')


Caused by: org.apache.spark.SparkException: Job aborted due to stage failure: Task 870 in stage 20.0 failed 4 times, most recent failure: Lost task 870.3 in stage 20.0 (TID 18684, worker04, executor 4): java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)
	at net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:252)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:211)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.closeAndGetSpills(ShuffleExternalSorter.java:419)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:230)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	at java.lang.Thread.run(Thread.java:748)

Driver stacktrace:
	at org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1925)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1913)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1912)
	at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)
	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)
	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1912)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)
	at org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:948)
	at scala.Option.foreach(Option.scala:257)
	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:948)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2146)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2095)
	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2084)
	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:759)
	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)
	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:167)
	... 35 more
Caused by: java.io.IOException: No space left on device
	at java.io.FileOutputStream.writeBytes(Native Method)
	at java.io.FileOutputStream.write(FileOutputStream.java:326)
	at org.apache.spark.storage.TimeTrackingOutputStream.write(TimeTrackingOutputStream.java:58)
	at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:82)
	at java.io.BufferedOutputStream.write(BufferedOutputStream.java:126)
	at net.jpountz.lz4.LZ4BlockOutputStream.flushBufferedData(LZ4BlockOutputStream.java:220)
	at net.jpountz.lz4.LZ4BlockOutputStream.write(LZ4BlockOutputStream.java:173)
	at org.apache.spark.storage.DiskBlockObjectWriter.write(DiskBlockObjectWriter.scala:252)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.writeSortedFile(ShuffleExternalSorter.java:211)
	at org.apache.spark.shuffle.sort.ShuffleExternalSorter.closeAndGetSpills(ShuffleExternalSorter.java:419)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.closeAndWriteOutput(UnsafeShuffleWriter.java:230)
	at org.apache.spark.shuffle.sort.UnsafeShuffleWriter.write(UnsafeShuffleWriter.java:190)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:99)
	at org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:55)
	at org.apache.spark.scheduler.Task.run(Task.scala:123)
	at org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:408)
	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:414)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)
	... 1 more


# --------------------------------------------------------------------------
# Change the spark temporary directory to the Volume storage directory
# fedora@zeppelin
nano /opt/spark/conf/spark-defaults.conf
	..
	spark.local.dir         /data-01/hdfs/data/
        ..




# Restart Zeppelin
# --------------------------------------------------------------------------
# fedora@zeppelin
/home/fedora/zeppelin-0.8.2-bin-all/bin/zeppelin-daemon.sh  restart



# ------------------------------------------------
# Run Pyspark cells and check /data-01/hdfs/data/
# fedora@zeppelin

ls -al /data-01/hdfs/data/
total 16
drwxrwsr-x.  4 fedora fedora 4096 Jan 27 17:53 .
drwxrwsr-x.  3 fedora fedora 4096 Jan 26 14:32 ..
drwxrwsr-x. 14 fedora fedora 4096 Jan 27 17:56 blockmgr-0a557731-b8f9-4165-8d05-e6169610c4a8
drwx--S---.  4 fedora fedora 4096 Jan 27 17:54 spark-ed2cd656-f229-490d-bca8-c5b4d27e7fdb


# ------------------------------------------------
# Run Full Pyspark Bulk Data loading example
# Completes but takes a long time
# ~ 9 hrs


# ------------------------------------------------
# Run Good astrometric solutions via ML Random Forrest classifier
# Change numTrees to 5000 (ngh mentioned this values causes the out of disk space issue)

# Completes Successfully







