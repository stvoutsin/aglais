#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#  
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
#
# </meta:header>
#
#




Target:

	Test Forrest Classifier ML Notebook using Cinder Volumes as tmp Storage for Spark & Hadoop and with local disk 
        In this test we run two separate deploys, with the only difference being where Spark & Hadoop store tmp data
        We then compare the timings of our sample notebook for each.

 
Result:

	Ongoing test..


# ----------------------------[Test 1: wfau/master Local disk for tmp]-------------------------




# -----------------------------------------------------
# Setup an Ansible deploy 
#[user@desktop]

    cloudname=gaia-test

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator \
        --hostname kubernator \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash




# -----------------------------------------------------
# Delete everything.
#[root@kubernator]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Default locations for config and status.
#[root@kubernator]

    configyml=/tmp/aglais-config.yml
    statusyml=/tmp/aglais-status.yml


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@kubernator]

cat > "${configyml:?}" << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloudname: ${cloudname:?}
EOF


# -----------------------------------------------------
# Create everything.
# This first deploy creates a cluster that uses the Cinder Volumes for temp storage for Spark & Hadoop


#[root@kubernator]

    /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....



PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   






# ------------------------------------------------------------------------------------------------------	
# Open prototype Zeppelin service and test the Forrest Classifier Notebook 
# https://github.com/wfau/aglais-testing/blob/main/notebooks/Good_astrometric_solutions_via_ML_Random_Forrest_classifier.json

# Make sure the the numTrees is 500
# rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)




	
# Raw Sources Cell 
# ------------------------------------------------------
%spark.pyspark

# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)
sqlContext.clearCache()

# a conservative selection of everything that COULD be within 100pc, including things with measured 
# distances putting them outside the 100pc horizon when their true distances are within, and also including 
# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:
raw_sources_df = spark.sql('SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, ' + photometric_consistency_indicators + features_select_string + ' FROM gaia_source WHERE ABS(parallax) > 8.0')

# cache it for speedy access below (all subsequent samples are derived from this):
raw_sources_df.cache()

# register as SQL-queryable
raw_sources_df.createOrReplaceTempView('raw_sources')

raw_sources_df.count()
# EDR3: 1,724,028 sources in 10min 21sec
# (cf. GCNS: 1,211,740 sources with varpi > 8mas plus 512,288 sources with varpi < -8 = 1,724,028 in total) 


	>  Took 9 min 56 sec. Last updated by admin at February 13 2021, 9:09:38 PM. 






# Forest Classifier Cell 
# ------------------------------------------------------


%spark.pyspark

# This cell does the business, given the data and training sets. Follows the example Python code at 
# https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier

from pyspark.ml.classification import RandomForestClassifier

# instantiate a trained RF classifier, seeded for repeatability at this stage:
rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)
model = rf.fit(training_df)


	> Took 15 min 32 sec. Last updated by admin at February 13 2021, 9:25:00 PM.







# ----------------------------[Test 2: stvoutsin/issue-288  /  Cinder Volume for tmp  / 500 trees]----------------------------


# Deploy with branch issue-288 on github/stvoutsin, which creates and uses directories on Cinder Volumes for temporary Spark & Hadoop data


# -----------------------------------------------------
# Setup an Ansible deploy 
#[user@desktop]

    cloudname=gaia-test

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator \
        --hostname kubernator \
        --env "SSH_AUTH_SOCK=/mnt/ssh_auth_sock" \
        --volume "${SSH_AUTH_SOCK}:/mnt/ssh_auth_sock:rw,z" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/common:/common:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/hadoop-yarn:/hadoop-yarn:ro,z" \
        atolmis/ansible-client:latest \
        bash




# -----------------------------------------------------
# Delete everything.
#[root@kubernator]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Default locations for config and status.
#[root@kubernator]

    configyml=/tmp/aglais-config.yml
    statusyml=/tmp/aglais-status.yml


# -----------------------------------------------------
# Create our Aglais configuration.
#[root@kubernator]

cat > "${configyml:?}" << EOF
aglais:
    version: 1.0
    spec:
        openstack:
            cloudname: ${cloudname:?}
EOF


# -----------------------------------------------------
# Create everything.
# This first deploy creates a cluster that uses the Cinder Volumes for temp storage for Spark & Hadoop


#[root@kubernator]

    /hadoop-yarn/bin/create-all.sh

    >   ....
    >   ....



PLAY RECAP **************************************************************************************************************************************************************************************************
gateway                    : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
master01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker01                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker02                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker03                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker04                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker05                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
worker06                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   
zeppelin                   : ok=4    changed=3    unreachable=0    failed=0    skipped=0    rescued=0    ignored=0   





# ------------------------------------------------------------------------------------------------------	
# Open prototype Zeppelin service and test the Forrest Classifier Notebook 
# https://github.com/wfau/aglais-testing/blob/main/notebooks/Good_astrometric_solutions_via_ML_Random_Forrest_classifier.json

# Set the numTrees to 500
# rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)




# ------------------------- Results --------------------------


# For comparison, benchmarking purposes, I'm using two cells that take the most time to complete
# Results (duration) below



	
# Raw Sources Cell 
# ------------------------------------------------------

%spark.pyspark

# clear any previously cached data in the context (cells may be executed in any order, and out-dated by changes from here onwards)
sqlContext.clearCache()

# a conservative selection of everything that COULD be within 100pc, including things with measured 
# distances putting them outside the 100pc horizon when their true distances are within, and also including 
# loads of spurious chaff with the wheat of course, plus bad things with significant, unphysical parallaxes:
raw_sources_df = spark.sql('SELECT source_id, random_index, phot_g_mean_mag, phot_bp_rp_excess_factor, bp_rp, g_rp, parallax, ra, dec, b, ' + photometric_consistency_indicators + features_select_string + ' FROM gaia_source WHERE ABS(parallax) > 8.0')

# cache it for speedy access below (all subsequent samples are derived from this):
raw_sources_df.cache()

# register as SQL-queryable
raw_sources_df.createOrReplaceTempView('raw_sources')

raw_sources_df.count()
# EDR3: 1,724,028 sources in 10min 21sec
# (cf. GCNS: 1,211,740 sources with varpi > 8mas plus 512,288 sources with varpi < -8 = 1,724,028 in total

    >  Took 10 min 51 sec. Last updated by admin at February 14 2021, 12:47:49 PM.




# Forrest Classifier Cell 
# ------------------------------------------------------


%spark.pyspark

# This cell does the business, given the data and training sets. Follows the example Python code at 
# https://spark.apache.org/docs/2.4.7/api/python/pyspark.ml.html#pyspark.ml.classification.RandomForestClassifier

from pyspark.ml.classification import RandomForestClassifier

# instantiate a trained RF classifier, seeded for repeatability at this stage:
rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 500, impurity = 'gini', seed=42)
model = rf.fit(training_df)

    > Took 16 min 11 sec. Last updated by admin at February 14 2021, 1:04:14 PM.


# ----------------------------[Test 2b: stvoutsin/issue-288  /  Cinder Volume for tmp  / 5000 trees]----------------------------

# After changing the numTrees to 5000:
# rf = RandomForestClassifier(featureSubsetStrategy = 'sqrt', featuresCol = 'features', labelCol = 'label', numTrees = 5000, impurity = 'gini', seed=42)

Took 1 hrs 23 min 50 sec. Last updated by admin at February 14 2021, 7:44:23 PM.

