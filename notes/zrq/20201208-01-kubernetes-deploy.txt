#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2020, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#


    Target:

        Run the Kubernetes deploy

    Result:

        Works :-)

    TODO:

        Add the /data and /user shares to all the interpreter nodes.
        Hide the /data and /user shares in the Spark worker params.

# -----------------------------------------------------
# Update the project name.
#[user@desktop]

    cloudname=gaia-prod

    sed -i '
        s/^\(AGLAIS_CLOUD\)=.*$/\1='${cloudname:?}'/
        ' "${HOME}/aglais.env"

# -----------------------------------------------------
# Create a container to work with.
#[user@desktop]

    source "${HOME:?}/aglais.env"

    podman run \
        --rm \
        --tty \
        --interactive \
        --name kubernator \
        --hostname kubernator \
        --env "clouduser=${AGLAIS_USER:?}" \
        --env "cloudname=${AGLAIS_CLOUD:?}" \
        --volume "${HOME:?}/clouds.yaml:/etc/openstack/clouds.yaml:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/openstack:/openstack:ro,z" \
        --volume "${AGLAIS_CODE:?}/experiments/kubernetes:/kubernetes:rw,z" \
        atolmis/openstack-client:latest \
        bash

# -----------------------------------------------------
# Delete everything.
#[root@kubernator]

    /openstack/bin/delete-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Run the main Kubernetes deployment.
#[root@kubernator]

    /kubernetes/bin/create-all.sh \
        "${cloudname:?}"

    >   ....
    >   ....


# -----------------------------------------------------
# Get the ServiceAccount token.
#[root@kubernator]

    buildname="aglais-k8s-$(date '+%Y%m%d')"
    namespace=${buildname,,}

    secretname=$(
        kubectl \
            --output json \
            --namespace "${namespace:?}" \
            get ServiceAccount \
                "aglais-dashboard-kubernetes-dashboard" \
        | jq -r '.secrets[0].name'
        )

    kubectl \
        --output json \
        --namespace "${namespace:?}" \
        get Secret \
            "${secretname:?}" \
    | jq -r '.data.token | @base64d'


    >   ....
    >   ....


# -----------------------------------------------------
# Get the Ingress address.
#[root@kubernator]

    kubectl \
        --namespace "${namespace:?}" \
        get Ingress

    >   NAME                                    HOSTS                  ADDRESS           PORTS   AGE
    >   aglais-dashboard-kubernetes-dashboard   valeria.metagrid.xyz   128.232.227.125   80      6m41s

# -----------------------------------------------------
# -----------------------------------------------------

    #
    # Update our DNS ..
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Login to Dashboard and test ...
#[user@desktop]

    firefox --new-window "https://valeria.metagrid.xyz/" &

    #
    # Yay - works :-)
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Login to Zeppelin and test ...
#[user@desktop]

    firefox --new-window "https://zeppelin.metagrid.xyz/" &

    #
    # Yay - works :-)
    #


# -----------------------------------------------------
# -----------------------------------------------------
# Mount the Gaia data in our Spark workers.
#[user@zeppelin]

# --------------------------------
%spark.conf

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.path        /data/gaia/dr2
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.options.claimName aglais-gaia-dr2-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.mount.path        /user/nch
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-nch.options.claimName aglais-user-nch-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.mount.path        /user/stv
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-stv.options.claimName aglais-user-stv-claim

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.mount.path        /user/zrq
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.mount.readOnly    false
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-user-zrq.options.claimName aglais-user-zrq-claim


# --------------------------------
%spark.pyspark

gaia_data = sqlContext.read.parquet(
    "/data/gaia/dr2"
    )

print("DF count: ",      gaia_data.count())
print("DF partitions: ", gaia_data.rdd.getNumPartitions())

    >   DF count:  1692919135
    >   DF partitions:  5985

# --------------------------------

%spark.pyspark

tmass_data = sqlContext.read.parquet(
    "/user/zrq/tmass/pqt"
    )

print("DF count: ",      tmass_data.count())
print("DF partitions: ", tmass_data.rdd.getNumPartitions())

    >   DF count:  470992878
    >   DF partitions:  290





