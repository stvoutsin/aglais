#
# <meta:header>
#   <meta:licence>
#     Copyright (c) 2021, ROE (http://www.roe.ac.uk/)
#
#     This information is free software: you can redistribute it and/or modify
#     it under the terms of the GNU General Public License as published by
#     the Free Software Foundation, either version 3 of the License, or
#     (at your option) any later version.
#
#     This information is distributed in the hope that it will be useful,
#     but WITHOUT ANY WARRANTY; without even the implied warranty of
#     MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
#     GNU General Public License for more details.
#
#     You should have received a copy of the GNU General Public License
#     along with this program.  If not, see <http://www.gnu.org/licenses/>.
#   </meta:licence>
# </meta:header>
#
#zrq-notes-time
#zrq-notes-indent
#zrq-notes-crypto
#zrq-notes-ansible
#zrq-notes-osformat
#zrq-notes-zeppelin
#

    Target:

        Mount each of the external catalogs in Spark.

    Result:

        Work in progress ..



# -----------------------------------------------------
# TODO - create a map of external catalogs and add it to our source.
#[user@zeppelin]

    catalogs:

      - name: "GEDR3"
        source: "/user/nch/PARQUET/TESTS/GEDR3"
        sharename: "aglais-gaia-edr3"
        sharesize: "540"
        mountpath: "/data/gaia/edr3"

      - name: "ALLWISE"
        source: "/user/nch/PARQUET/TESTS/ALLWISE"
        sharename: "aglais-wise-allwise"
        sharesize: "350"
        mountpath: "/data/wise/allwise"

      - name: "PS1"
        source: "/user/nch/PARQUET/TESTS/PS1"
        sharename: "aglais-panstarrs-dr1"
        sharesize: "300"
        mountpath: "/data/panstarrs/dr1"

      - name: "2MASS"
        source: "/user/nch/PARQUET/TESTS/2MASS"
        sharename: "aglais-twomass-allsky"
        sharesize: "40"
        mountpath: "/data/twomass/allsky"


# -----------------------------------------------------
# Mount each of the external catalogs in Spark.
#[user@zeppelin]

# --------------------------------
%spark.conf

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.path        /data/gaia/dr2
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-dr2.options.claimName aglais-gaia-dr2-claim

# --------------------------------
%spark.pyspark

gaia_dr2 = sqlContext.read.parquet(
    "/data/gaia/dr2"
    )

print("DF count: ",      gaia_dr2.count())
print("DF partitions: ", gaia_dr2.rdd.getNumPartitions())

    >   DF count:  1692919135
    >   DF partitions:  5985


# --------------------------------
%spark.conf

spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-edr3.mount.path        /data/gaia/edr3
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-edr3.mount.readOnly    true
spark.kubernetes.executor.volumes.persistentVolumeClaim.aglais-gaia-edr3.options.claimName aglais-gaia-edr3-claim

# --------------------------------
%spark.pyspark

gaia_edr3 = sqlContext.read.parquet(
    "/data/gaia/edr3"
    )

print("DF count: ",      gaia_edr3.count())
print("DF partitions: ", gaia_edr3.rdd.getNumPartitions())

    >   ---------------------------------------------------------------------------
    >   AnalysisException                         Traceback (most recent call last)
    >   <ipython-input-10-67bb2d311bd0> in <module>
    >         1 gaia_edr3 = sqlContext.read.parquet(
    >   ----> 2     "/data/gaia/edr3"
    >         3     )
    >         4
    >         5 print("DF count: ",      gaia_edr3.count())
    >   
    >   ~spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py in parquet(self, *paths, **options)
    >       351         self._set_opts(mergeSchema=mergeSchema, pathGlobFilter=pathGlobFilter,
    >       352                        recursiveFileLookup=recursiveFileLookup)
    >   --> 353         return self._df(self._jreader.parquet(_to_seq(self._spark._sc, paths)))
    >       354
    >       355     @ignore_unicode_prefix
    >   
    >   ~spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py in __call__(self, *args)
    >      1303         answer = self.gateway_client.send_command(command)
    >      1304         return_value = get_return_value(
    >   -> 1305             answer, self.gateway_client, self.target_id, self.name)
    >      1306
    >      1307         for temp_arg in temp_args:
    >   
    >   ~spark/python/lib/pyspark.zip/pyspark/sql/utils.py in deco(*a, **kw)
    >       135                 # Hide where the exception came from that shows a non-Pythonic
    >       136                 # JVM exception message.
    >   --> 137                 raise_from(converted)
    >       138             else:
    >       139                 raise
    >   
    >   ~spark/python/lib/pyspark.zip/pyspark/sql/utils.py in raise_from(e)
    >   
    >   AnalysisException: Path does not exist: file:/data/gaia/edr3;

    #
    # This doesn't work because we need to add the CSI Claim to the Zeppelin interpreter too.
    # At the moment, the list of VolumeClaims added to the interpreter is statically defined
    # in the Docker image.

    https://github.com/Zarquan/aglais/blob/20210111-zrq-working/experiments/kubernetes/docker/zeppelin/dev/k8s/interpreter/100-interpreter-spec.yaml#L67

      ....
      {% if zeppelin.k8s.interpreter.group.name == "spark" %}
        - name: spark-home
          mountPath: /spark
        - name: aglais-gaia-dr2
          mountPath: /data/gaia/dr2
          readOnly: true
        - name: aglais-user-nch
          mountPath: /user/nch
          readOnly: false
        - name: aglais-user-stv
          mountPath: /user/stv
          readOnly: false
        - name: aglais-user-zrq
          mountPath: /user/zrq
          readOnly: false
      {% endif %}
      ....


    https://github.com/Zarquan/aglais/blob/20210111-zrq-working/experiments/kubernetes/docker/zeppelin/dev/k8s/interpreter/100-interpreter-spec.yaml#L92

      ....
      volumes:
      {% if zeppelin.k8s.interpreter.group.name == "spark" %}
      - name: spark-home
        emptyDir: {}
      - name: aglais-gaia-dr2
        persistentVolumeClaim:
          claimName: aglais-gaia-dr2-claim
      - name: aglais-user-nch
        persistentVolumeClaim:
          claimName: aglais-user-nch-claim
      - name: aglais-user-stv
        persistentVolumeClaim:
          claimName: aglais-user-stv-claim
      - name: aglais-user-zrq
        persistentVolumeClaim:
          claimName: aglais-user-zrq-claim
      {% endif %}
      ....


    #
    # Which means we need to update the Docker image before we can mount these volumes.
    #


